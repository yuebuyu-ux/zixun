import asyncio
import json
import os
import re
import datetime
import argparse
import sys
import urllib.parse
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# =================é…ç½®åŒºåŸŸ=================
# è¯·åœ¨è¿™é‡Œå¡«å…¥ä½ çš„ LLM API Key
API_KEY = "ms-b8df244e-aa5e-4392-b3bf-4b0e0f80c052" 
API_BASE_URL = "https://api-inference.modelscope.cn/v1" 
MODEL_NAME = "ZhipuAI/GLM-4.7" 

# æ•°æ®æºé…ç½®
SOURCES = [
    {
        "name": "Tophub Daily",
        "url": "https://tophub.today/daily",
        "type": "manual_captcha", # éœ€è¦æ‰‹åŠ¨éªŒè¯
        "selector": "body"
    }
]
# =========================================

async def fetch_html_content(source):
    """
    é€šç”¨ HTML è·å–å™¨
    """
    print(f"æ­£åœ¨å°è¯•ä» [{source['name']}] è·å–å†…å®¹...")
    
    async with async_playwright() as p:
        # ç™¾åº¦çƒ­æœé€šå¸¸ä¸éœ€è¦å¤æ‚çš„éªŒè¯ï¼Œheadless=True å³å¯
        # Tophub éœ€è¦ headless=False
        headless = source['type'] != 'manual_captcha'
        
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            await page.goto(source['url'], timeout=60000)
            
            if source['type'] == 'manual_captcha':
                print("\n" + "="*50)
                print("ã€æ™ºèƒ½ç­‰å¾…åŠ è½½ã€‘")
                print("æ­£åœ¨ç­‰å¾…é¡µé¢åŠ è½½... å¦‚é‡éªŒè¯ç è¯·æ‰‹åŠ¨å®Œæˆã€‚")
                print("="*50 + "\n")
                
                # 1. å°è¯•è‡ªåŠ¨ç­‰å¾…åŠ è½½å®Œæˆ
                try:
                    await page.wait_for_load_state('networkidle', timeout=15000)
                except:
                    print("ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­æ£€æŸ¥å†…å®¹...")

                # 2. æ£€æŸ¥å½“å‰å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                temp_text = await page.inner_text("body")
                if len(temp_text) < 500 or "Just a moment" in temp_text:
                    print("æ£€æµ‹åˆ°å†…å®¹è¿‡çŸ­æˆ–åŒ…å«éªŒè¯æç¤ºï¼Œæš‚åœç­‰å¾…äººå·¥ä»‹å…¥...")
                    await asyncio.to_thread(input, ">> è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯å¹¶æ˜¾ç¤ºæ–°é—»åˆ—è¡¨åï¼ŒæŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")
                else:
                    print(f"é¡µé¢ä¼¼ä¹å·²åŠ è½½ (å†…å®¹é•¿åº¦: {len(temp_text)})ï¼Œè‡ªåŠ¨ç»§ç»­...")
                
                if "tophub.today" in source['url'] and "daily" in source['url']:
                    print("å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰ã€æ™šæŠ¥ã€‘å†…å®¹...")
                    try:
                        content_early = await page.inner_text("body")
                        print(f"ã€å½“å‰æ—©æŠ¥ã€‘å­—æ•°: {len(content_early)}")
                        print("ã€éœ€è¦äººå·¥ä»‹å…¥ã€‘è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç‚¹å‡»æ­£ç¡®çš„ã€æ™šæŠ¥ã€‘æŒ‰é’®ï¼Œç¡®è®¤å†…å®¹æ›´æ–°ã€‚")
                        await asyncio.to_thread(input, ">> æ‰‹åŠ¨åˆ‡æ¢å®Œæˆåï¼Œè¯·æŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")

                        content_final = await page.inner_text("body")
                        print(f"äººå·¥ç¡®è®¤åå†…å®¹å­—æ•°: {len(content_final)}")

                        if len(content_final) != len(content_early):
                            merged = f"=== æ—©æŠ¥å†…å®¹ ===\n{content_early}\n\n=== æ™šæŠ¥å†…å®¹ ===\n{content_final}"
                            print(f"åˆå¹¶åæ€»å­—æ•°: {len(merged)}")
                            return merged

                        print("å†…å®¹ä»æœªå˜åŒ–ï¼Œå°†ä»…ä½¿ç”¨å½“å‰å†…å®¹ã€‚")
                        return content_final

                    except Exception as e:
                        print(f"å°è¯•åˆ‡æ¢æ™šæŠ¥æ—¶å‡ºé”™: {e}ã€‚å°†ä»…ä½¿ç”¨å½“å‰å†…å®¹ã€‚")
                        pass
                # ========================================

            else:
                # è‡ªåŠ¨ç­‰å¾…
                print("ç­‰å¾…é¡µé¢åŠ è½½...")
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(3000)

            # è·å– HTML
            content = await page.content()
            
            # ä½¿ç”¨ BeautifulSoup æå–ä¸»è¦å†…å®¹ï¼Œå‡å°‘ token
            soup = BeautifulSoup(content, 'html.parser')
            
            # é’ˆå¯¹ä¸åŒæºçš„ç®€å•æ¸…ç†
            if "baidu" in source['url']:
                # ç™¾åº¦çƒ­æœçš„ä¸»è¦å†…å®¹åœ¨ main æˆ–ç‰¹å®š class ä¸­
                main_content = soup.find('div', class_='container') or soup.body
            else:
                main_content = soup.body
                
            # è½¬ä¸ºæ–‡æœ¬ (ä¿ç•™ä¸€å®šçš„ HTML ç»“æ„å¯èƒ½æ›´å¥½ï¼Œä½†çº¯æ–‡æœ¬æ›´çœ token)
            # è¿™é‡Œæˆ‘ä»¬ä¸ºäº†è®© LLM æ›´å¥½ç†è§£ç»“æ„ï¼Œæå–çº¯æ–‡æœ¬ä½†ä¿ç•™æ¢è¡Œ
            text_content = main_content.get_text(separator='\n', strip=True)
            
            print(f"æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(text_content)} å­—ç¬¦")
            return text_content
            
        except Exception as e:
            print(f"è·å–å¤±è´¥: {e}")
            return None
        finally:
            await browser.close()

def build_hot_items(text, max_items=120):
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    items = []
    i = 0
    while i < len(lines):
        if re.fullmatch(r"\d{1,3}", lines[i]):
            try:
                rank = int(lines[i])
            except Exception:
                i += 1
                continue
            title = lines[i + 1] if i + 1 < len(lines) else ""
            j = i + 2
            source = ""
            heat = ""
            if j < len(lines) and ("Â·" in lines[j] or "â€§" in lines[j]):
                source = lines[j].replace("Â·", "").replace("â€§", "").strip()
                j += 1
            if j < len(lines) and ("çƒ­åº¦" in lines[j] or re.search(r"\d", lines[j])):
                heat = lines[j].strip()
                j += 1
            if title and not re.fullmatch(r"\d{1,3}", title):
                items.append({"rank": rank, "title": title, "source": source, "heat": heat})
            i = j
        else:
            i += 1

    if not items:
        candidate_lines = []
        for line in lines[:1000]:
            if len(line) < 6:
                continue
            if "çƒ­åº¦" in line:
                continue
            candidate_lines.append(line)
        items = [{"rank": idx + 1, "title": t, "source": "", "heat": ""} for idx, t in enumerate(candidate_lines[:max_items])]

    return items[:max_items]

def build_hot_digest(text, max_items=120):
    items = build_hot_items(text, max_items=max_items)
    digest_lines = []
    for item in items[:max_items]:
        meta = []
        if item.get("source"):
            meta.append(item["source"])
        if item.get("heat") and "çƒ­åº¦" in item["heat"]:
            meta.append(item["heat"])
        meta_str = f"ï¼ˆ{'ï¼Œ'.join(meta)}ï¼‰" if meta else ""
        digest_lines.append(f"{item['rank']}. {item['title']}{meta_str}")
    return "\n".join(digest_lines)

def _get_env(name, default_value):
    value = os.environ.get(name)
    if value is None:
        return default_value
    value = value.strip()
    return value if value else default_value

def _make_openai_client():
    try:
        from openai import OpenAI
    except ImportError:
        return None
    api_key = _get_env("API_KEY", API_KEY)
    api_base_url = _get_env("API_BASE_URL", API_BASE_URL)
    return OpenAI(api_key=api_key, base_url=api_base_url)

def _clean_json_text(text):
    result = (text or "").replace("```json", "").replace("```", "").strip()
    result = re.sub(r':\s*â€œ', ': "', result)
    result = re.sub(r'â€\s*,', '",', result)
    result = re.sub(r'â€\s*}', '"}', result)
    result = re.sub(r'â€\s*]', '"]', result)
    return result

async def process_with_llm(text):
    """
    è°ƒç”¨ LLM API å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æå’Œæ€»ç»“ (ä»¿ç…§ 60s-static-host æ€è·¯)
    """
    client = _make_openai_client()
    if not client:
        print("é”™è¯¯ï¼šæœªå®‰è£… openai åº“ã€‚")
        return None
    
    # è®¡ç®—å‘å¸ƒæ—¥æœŸï¼ˆæ˜å¤©ï¼‰
    publish_date = datetime.datetime.now() + datetime.timedelta(days=1)
    publish_date_str = publish_date.strftime("%Yå¹´%mæœˆ%dæ—¥")

    prompt = f"""
è¯·å°†ä»¥ä¸‹æ–°é—»å†…å®¹æå–ä¸º JSON æ ¼å¼ï¼Œé€‚é…æ—©é—´å…¬ä¼—å·æ–°é—»æ€»ç»“åœºæ™¯ï¼Œçªå‡ºâ€œæƒå¨ã€å®ç”¨ã€æ˜“è¯»â€ã€‚

ã€æ—¥æœŸè§„åˆ™ã€‘
1. æœ¬å†…å®¹å°†äº **{publish_date_str}** å‘å¸ƒã€‚
2. æ–‡ä¸­æ‰€æœ‰æ¶‰åŠæ—¶é—´çš„å†…å®¹ï¼Œ**å¿…é¡»è½¬æ¢ä¸ºç»å¯¹æ—¥æœŸ**ï¼ˆå¦‚â€œ1æœˆ18æ—¥â€ï¼‰ï¼Œ**ä¸¥ç¦**ä½¿ç”¨â€œæ˜æ—¥â€ã€â€œæ˜å¤©â€ã€â€œä¸‹å‘¨â€ã€â€œåå¤©â€ç­‰ç›¸å¯¹æ—¶é—´åè¯ï¼Œé¿å…è¯»è€…äº§ç”Ÿæ—¶é—´é”™ä¹±ã€‚
3. å¦‚æœåŸæ–‡è¯´æ˜¯â€œæ˜å¤©å¼€å§‹â€ï¼Œè¯·æ ¹æ®å½“å‰æ—¥æœŸæ¨ç®—å…·ä½“æ˜¯å“ªä¸€å¤©å¹¶å†™æ˜ã€‚

ã€ç­›é€‰è§„åˆ™ã€‘
1.  ä¼˜å…ˆé€‰æ‹© **æ°‘ç”Ÿæ”¿ç­–ã€è¡Œä¸šå¤§äº‹ã€æ­£èƒ½é‡ç¤¾ä¼šæ–°é—»ã€é‡è¦ç§‘æŠ€/ç»æµåŠ¨æ€**ï¼›
2.  å‰”é™¤å†…å®¹ï¼šç¾éš¾äº‹æ•…ã€è´Ÿé¢æš´åŠ›ã€æ•æ„Ÿæ”¿æ²»ã€æ— æ„ä¹‰å…«å¦ï¼›
3.  ç­›é€‰æ ‡å‡†ï¼šä¼˜å…ˆä»å½“æ—¥çƒ­æœTop30ã€æƒå¨åª’ä½“å¤´ç‰ˆä¸­é€‰å–ï¼Œç¡®ä¿15æ¡å†…å®¹è¦†ç›–å¤šé¢†åŸŸï¼ˆä¸é‡å¤åŒä¸€ä¸»é¢˜ï¼‰ã€‚
4.  å†…å®¹å”¯ä¸€æ€§è¦æ±‚ï¼šä¸¥ç¦å‡ºç°æ ‡é¢˜ã€æ ¸å¿ƒäº‹å®ï¼ˆå¦‚äº‹ä»¶ä¸»ä½“ã€å…³é”®æ•°æ®ã€æ—¶é—´ï¼‰å®Œå…¨ä¸€è‡´çš„é‡å¤å†…å®¹ï¼Œç›¸ä¼¼ä¸»é¢˜éœ€å·®å¼‚åŒ–è¡¨è¿°ï¼ˆå¦‚ä¸åŒåœ°åŒºçš„æ°‘ç”Ÿæ”¿ç­–ï¼‰ï¼›
5.  æ’åºè¦æ±‚ï¼šå°†å›½å®¶æ”¿ç­–ã€é‡è¦æ°‘ç”Ÿæ”¿ç­–ã€é‡å¤§è¡Œä¸šæ”¿ç­–ç›¸å…³å†…å®¹æ”¾åœ¨æœ€å‰é¢è¾“å‡ºï¼Œå…¶æ¬¡å†è¾“å‡ºå…¶ä»–ç±»åˆ«ã€‚
6.  å…œåº•æœºåˆ¶ï¼šè‹¥å½“æ—¥ç¬¦åˆè¦æ±‚çš„æœ‰æ•ˆæ–°é—»ä¸è¶³15æ¡ï¼ŒæŒ‰å®é™…æ•°é‡è¾“å‡ºï¼ˆæ— éœ€å‡‘æ•°ï¼‰ï¼Œä¼˜å…ˆä¿ç•™é«˜å…³æ³¨åº¦é¢†åŸŸå†…å®¹ã€‚
ã€æ ¼å¼è¦æ±‚ã€‘
1.  æä¾›ä¸€ä¸ªæ ‡é¢˜å­—æ®µ "page_title"ï¼š
    - ç»“æ„ï¼š60s çœ‹æ‡‚ä¸–ç•Œ + ï½œ + é«˜å…³æ³¨åº¦å…³é”®è¯ï¼ˆ1-2 ä¸ªï¼‰ + æƒ…ç»ªè¯ / ä»·å€¼ç‚¹ï¼ˆå¦‚å½±å“æ°‘ç”Ÿ / å…³é”®è¿›å±• / åº”å¯¹æŒ‡å—ï¼‰
    - å­—æ•°ï¼š18-25 å­—ï¼Œæ‰‹æœºç«¯æ˜¾ç¤ºå®Œæ•´ï¼Œé¿å… â€œä»Šæ—¥æ–°é—»æ±‡æ€»â€ ç±»å¹³æ·¡è¡¨è¿°
    - é£æ ¼ï¼šç®€æ´æœ‰åŠ›ï¼Œå¸¦æ­£å‘å¼•å¯¼ï¼Œçªå‡º â€œå®ç”¨æ€§â€ æˆ– â€œçªç ´æ€§â€ï¼Œå¯é€‚å½“ç”¨ â€œï¼â€ å¼ºåŒ–æƒ…ç»ª
2.  ç”Ÿæˆ 1 ä¸ªå¼€å¤´æ–‡æ¡ˆå­—æ®µ "opening"ï¼š
    - é£æ ¼ï¼šå¹²ç»ƒå®ç”¨ï¼Œç¬¦åˆæ—©é—´å¿«é€Ÿè¯»æ–°é—»çš„èŠ‚å¥ï¼Œ30-45 å­—ï¼ˆä¸åŒ…å«è¿½åŠ å¥ï¼‰
    - å†…å®¹ï¼šæ—¶é—´æé†’ + 1-2 ä¸ªæ ¸å¿ƒæ–°é—»é’©å­ï¼ˆç»‘å®šæ°‘ç”Ÿ / åˆ©ç›Šç‚¹ï¼‰+ é˜…è¯»å¼•å¯¼ï¼ˆç‚¹æ˜é«˜æ•ˆï¼‰+ è¿½åŠ å¥ â€œå…ˆä¸Šæ±‡æ€»å›¾ï¼15 ä¸ªçƒ­ç‚¹æ ‡é¢˜ + ä¸¤å¥è¯ç²¾åéƒ½åœ¨è¿™ï¼Œåˆ·å®Œå›¾å†çœ‹è¯¦æï¼Œä¸æµªè´¹ä½ ä¸€ç§’é’Ÿâ€ï¼Œè‡ªç„¶è¡”æ¥ä¸ç”Ÿç¡¬
    - ç‚¹ç¼€ï¼šå¯åŠ  1 ä¸ªæç¤ºç±» emojiï¼ˆå¦‚ğŸ“Œ/â°ï¼‰ï¼Œæ•´ä½“çªå‡º â€œçœæ—¶é—´ã€æœ‰ä»·å€¼â€
3.  ç”Ÿæˆ 1 ä¸ªç»“å°¾æ–‡æ¡ˆå­—æ®µ "ending"ï¼š
    - é£æ ¼ï¼šæ¸©å’Œæ­£å‘ï¼Œå¸¦å¼•å¯¼æ€§ï¼Œ40-55å­—
    - å†…å®¹ï¼šæ€»ç»“ä»·å€¼+äº’åŠ¨æé—®ï¼ˆå¯é€‰ï¼‰+å…³æ³¨å¼•å¯¼
    - ç‚¹ç¼€ï¼šå¯åŠ 1ä¸ªå¼•å¯¼ç±»emojiï¼ˆå¦‚â­/ğŸ””ï¼‰
4.  æå– 15 æ¡æ–°é—»ï¼Œæ”¾å…¥ "news_items" æ•°ç»„ä¸­ï¼ˆä¸è¶³åˆ™æŒ‰å®é™…æ•°é‡è¾“å‡ºï¼‰ã€‚æ¯æ¡å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š
    - "title"ï¼šæ–°é—»æ ‡é¢˜ï¼ˆâ‰¤20å­—ï¼Œç®€ç»ƒæœ‰åŠ›ï¼Œæå–æ ¸å¿ƒäº‹å®/æ•°æ®/çªç ´ç‚¹ï¼Œä¸å †ç Œä¿®é¥°è¯ï¼‰ï¼›
    - "summary_short"ï¼šå›¾ç‰‡é…å¥—æ–‡æ¡ˆï¼ˆ24-32å­—ï¼Œå¿…é¡»ä¸€è¡Œæ˜¾ç¤ºå®Œæ•´ï¼‰ï¼š
        - ç¬¬ä¸€å¥ï¼šç‚¹å‡ºæ–°é—»æ ¸å¿ƒçœ‹ç‚¹/å…³é”®å˜åŒ–ï¼ˆä¸é‡å¤æ ‡é¢˜ï¼‰ï¼›
        - ç¬¬äºŒå¥ï¼šç»™å‡ºä¸€å¥è§£è¯»/å½±å“/å¯ç¤ºï¼ˆå¸¦ä¸€ç‚¹è§‚ç‚¹ï¼‰ï¼›
        - å¥é—´ç”¨â€œï¼›â€åˆ†éš”ï¼Œ
    - "summary_long"ï¼šæ·±åº¦åˆ†æä¸èƒŒæ™¯è¡¥å……ï¼ˆ100-120å­—ï¼‰ï¼š
        - å†…å®¹ï¼šå……å®æœ‰è§‚ç‚¹ï¼ŒåŒ…å«èƒŒæ™¯ã€ç»†èŠ‚ã€å½±å“ï¼›
        - é£æ ¼ï¼šå®¢è§‚æƒå¨ï¼Œé¿å…å£è¯­åŒ–ã€‚
5.  æå–ä¸€å¥æœ€æœ‰å“²ç†çš„â€œé‡‘å¥â€ä½œä¸º "tip"ï¼š
    - é£æ ¼ï¼šæ­£å‘æœ‰æ·±åº¦ï¼Œè´´åˆæ—©é—´é˜…è¯»æ°›å›´ï¼›
    - å­—æ•°ï¼š15-20å­—ï¼Œä¸å¸¦ä½œè€…ï¼Œé¿å…é¸¡æ±¤åŒ–ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- åªè¿”å›çº¯ JSON æ ¼å¼ï¼Œæ— ä»»ä½• Markdown æ ‡è®°ã€æ— å¤šä½™è§£é‡Šæ–‡å­—ï¼›
- è¯­è¨€é£æ ¼ç»Ÿä¸€ï¼šæ­£å¼ä¸”æ˜“æ‡‚ï¼Œé€‚é…æ—©é—´æ–°é—»çš„æƒå¨æ„Ÿä¸å®ç”¨æ€§ã€‚

ã€å†…å®¹ã€‘
{text[:15000]}
"""

    print("æ­£åœ¨è¯·æ±‚ AI è¿›è¡Œæ™ºèƒ½è§£æ...")
    try:
        response = client.chat.completions.create(
            model=_get_env("MODEL_NAME", MODEL_NAME),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»ç¼–è¾‘ä¸è¯„è®ºå‘˜ï¼Œæ“…é•¿ç”¨ç®€çŸ­ä¸¤å¥è¯ç»™å‡ºè®²è§£ä¸è§‚ç‚¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        
        result = response.choices[0].message.content
        result = _clean_json_text(result)
        
        # print(f"DEBUG: LLM åŸå§‹å“åº”:\n{result}\n")
        
        data = json.loads(result)
        if not isinstance(data, dict):
            data = {"news_items": []}

        if isinstance(data.get("page_title"), dict):
            data["page_title"] = data["page_title"].get("text") or data["page_title"].get("title") or ""
        if "page_title" not in data:
            data["page_title"] = data.get("headline") or data.get("title_text") or ""
        if not isinstance(data.get("page_title"), str):
            data["page_title"] = ""
        data["page_title"] = data["page_title"].strip()
        if len(data["page_title"]) > 25:
            data["page_title"] = data["page_title"][:25]

        if isinstance(data.get("opening"), dict):
            data["opening"] = data["opening"].get("text") or data["opening"].get("content") or ""
        if "opening" not in data:
            data["opening"] = data.get("intro") or data.get("lead") or ""
        if not isinstance(data.get("opening"), str):
            data["opening"] = ""
        data["opening"] = data["opening"].strip()

        if isinstance(data.get("ending"), dict):
            data["ending"] = data["ending"].get("text") or data["ending"].get("content") or ""
        if "ending" not in data:
            data["ending"] = data.get("outro") or data.get("closing") or ""
        if not isinstance(data.get("ending"), str):
            data["ending"] = ""
        data["ending"] = data["ending"].strip()
        
        # å…¼å®¹æ€§å¤„ç†ï¼šå¯»æ‰¾å¯èƒ½çš„åˆ—è¡¨é”®å
        if 'news_items' not in data:
            for key in ['news', 'items', 'list', 'data', 'contents']:
                if key in data and isinstance(data[key], list):
                    data['news_items'] = data[key]
                    break
            
        # æ ‡å‡†åŒ–ï¼šç¡®ä¿ news_items æ˜¯å¯¹è±¡åˆ—è¡¨
        if 'news_items' in data and isinstance(data['news_items'], list):
            normalized_items = []
            for item in data['news_items']:
                if isinstance(item, str):
                    # æ—§æ ¼å¼å…¼å®¹ï¼ˆè™½ç„¶ Prompt è¦æ±‚äº†æ–°æ ¼å¼ï¼Œé˜²ä¸‡ä¸€ï¼‰
                    parts = item.split('ï½œ')
                    title = parts[0]
                    summary = parts[1] if len(parts) > 1 else ""
                    normalized_items.append({
                        "title": title,
                        "summary_short": summary[:30], # æˆªæ–­ä½œä¸ºçŸ­æ‘˜è¦
                        "summary_long": summary # ä½œä¸ºé•¿æ‘˜è¦
                    })
                elif isinstance(item, dict):
                    # ç¡®ä¿å­—æ®µé½å…¨
                    if "summary_short" not in item:
                        item["summary_short"] = item.get("summary", "")[:30]
                    if "summary_long" not in item:
                        item["summary_long"] = item.get("summary", "")
                    normalized_items.append(item)
            data['news_items'] = normalized_items
        
        if not data.get('news_items'):
             print("è­¦å‘Š: æœªèƒ½ä» AI å“åº”ä¸­æå–åˆ°ä»»ä½•æ–°é—»æ¡ç›® (news_items ä¸ºç©º)ã€‚")
             print(f"AI è¿”å›çš„é¡¶å±‚é”®: {list(data.keys())}")

        # å…¼å®¹æ€§å¤„ç†ï¼šå°† tip æ˜ å°„ä¸º quote
        if 'tip' in data and 'quote' not in data:
            data['quote'] = {"text": data['tip'], "author": ""}

        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœ quote æ˜¯å­—ç¬¦ä¸²è€Œä¸æ˜¯å¯¹è±¡
        if isinstance(data.get('quote'), str):
            data['quote'] = {"text": data['quote'], "author": ""}
        
        return data
    except json.JSONDecodeError:
        print(f"JSON è§£æå¤±è´¥ã€‚åŸå§‹å“åº”: {result}")
        # å°è¯•ä½¿ç”¨æ­£åˆ™æå–ç»“æ„åŒ–æ•°æ®ï¼ˆå…œåº•ï¼‰
        try:
            import re
            # å°è¯•åŒ¹é…å®Œæ•´çš„ item ç»“æ„
            item_pattern = r'"title":\s*"(.*?)".*?"summary_short":\s*"(.*?)".*?"summary_long":\s*"(.*?)"'
            matches = re.findall(item_pattern, result, re.S)
            
            if matches:
                print(f"é€šè¿‡æ­£åˆ™æ‰¾å› {len(matches)} æ¡ç»“æ„åŒ–æ–°é—»")
                news_items = []
                for m in matches:
                    news_items.append({
                        "title": m[0],
                        "summary_short": m[1],
                        "summary_long": m[2]
                    })
                
                # å°è¯•æå– tip
                tip_match = re.search(r'"tip":\s*"(.*?)"', result)
                tip_text = tip_match.group(1) if tip_match else "ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ·ã€‚"
                
                return {
                    "news_items": news_items,
                    "quote": {"text": tip_text, "author": ""}
                }
            
            # å¦‚æœç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²å¹¶å°½åŠ›æ‹¼å‡‘
            print("æ­£åˆ™ç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²...")
            all_strings = re.findall(r'"([^"]+)"', result)
            if all_strings and len(all_strings) > 5:
                 # å‡è®¾å‰å‡ ä¸ªæ˜¯ keysï¼Œæˆ‘ä»¬å¾ˆéš¾å‡†ç¡®æ¢å¤ï¼Œè¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨é¿å…å´©æºƒ
                 return {
                    "news_items": [{"title": "æ•°æ®è§£æé”™è¯¯", "summary_short": "è¯·æ£€æŸ¥æ—¥å¿—", "summary_long": f"åŸå§‹æ•°æ®ç‰‡æ®µ: {all_strings[:3]}..."}],
                    "quote": {"text": "ç³»ç»Ÿæ•…éšœ", "author": "Error"}
                 }
        except Exception as e:
            print(f"å…œåº•è§£æä¹Ÿå¤±è´¥: {e}")
            pass
        return None
    except Exception as e:
        print(f"AI åˆ†æå¤±è´¥: {e}")
        return None

async def format_with_llm(data):
    client = _make_openai_client()
    if not client:
        print("é”™è¯¯ï¼šæœªå®‰è£… openai åº“ã€‚")
        return None
    payload = {
        "page_title": data.get("page_title", ""),
        "opening": data.get("opening", ""),
        "ending": data.get("ending", ""),
        "quote": data.get("quote", {}),
        "news_items": data.get("news_items", [])[:15]
    }

    prompt = f"""
è¯·æŠŠä¸‹é¢ç»“æ„åŒ–å†…å®¹æ’ç‰ˆä¸º Markdownï¼Œé£æ ¼ä¸ºâ€œ60s è½»è¯»æ‰‹è®°â€æ—©æŠ¥ã€‚
ä¸¥æ ¼è¦æ±‚ï¼š
1. åªè¾“å‡º Markdownï¼Œä¸è¦è¾“å‡ºå¤šä½™è§£é‡Šæ–‡å­—ã€‚
2. ç¬¬ä¸€è¡Œä½¿ç”¨ä¸€çº§æ ‡é¢˜ï¼Œæ ¼å¼ä¸ºï¼š# {payload["page_title"]}ã€‚
3. ç¬¬äºŒè¡Œæ˜¯å¹²ç»ƒå¼•å¯¼è¯­ï¼ŒåŒ…å« emojiï¼Œå¹¶æé†’â€œå…ˆä¸Šæ±‡æ€»å›¾â€ï¼Œè¯­æ°”æƒå¨å®ç”¨ï¼Œæ ¼å¼ï¼š> å†…å®¹ã€‚
4. æ–°é—»æ ‡é¢˜ä½¿ç”¨äºŒçº§æ ‡é¢˜æ ¼å¼ï¼š## ğŸ”¸ åºå·. æ–°é—»æ ‡é¢˜ã€‚
5. æ¯æ¡æ–°é—»æ ‡é¢˜ä¸‹ä¸€è¡Œè¾“å‡º 1 ä¸ªå…³é”®è¯æ ‡ç­¾ï¼ˆå¦‚ #è´¢ç»ï¼‰ï¼Œå†ä¸‹ä¸€è¡Œä½¿ç”¨æ­£æ–‡æ ¼å¼ï¼Œæ­£æ–‡åŸºäº summary_longï¼Œå¯é€‚åº¦æ¶¦è‰²ä½†ä¸æ·»åŠ æ–°äº‹å®ã€‚
6. ä¿æŒ news_items çš„åŸå§‹é¡ºåºè¾“å‡ºï¼Œæ¯æ¡æ–°é—»ä¹‹é—´ç©ºä¸€è¡Œå¢å¼ºå¯è¯»æ€§ã€‚
7. ç»“å°¾ç”¨åˆ†å‰²çº¿ --- ï¼Œä¸‹ä¸€è¡Œè¾“å‡ºç»“å°¾æ–‡æ¡ˆï¼Œä¸è¦æ·»åŠ â€œä»Šæ—¥äº’åŠ¨â€å­—æ ·ã€‚
8. å†ä¸‹ä¸€è¡Œè¾“å‡ºé‡‘å¥ï¼ˆæ¥è‡ª quote.textï¼‰ï¼Œå¯çœç•¥ä½œè€…ï¼Œä¸è¦é¢å¤–å¼•å·ã€‚
9. ä»…åœ¨æ¯æ¡æ–°é—»ä¹‹é—´æ·»åŠ ä¸€è¡Œç©ºè¡Œï¼Œå…¶ä½™ä½ç½®ä¸å‡ºç°ç©ºç™½è¡Œã€‚

å†…å®¹ï¼š
{json.dumps(payload, ensure_ascii=False)}
"""

    print("æ­£åœ¨è¯·æ±‚ AI è¿›è¡Œ Markdown æ’ç‰ˆ...")
    try:
        response = client.chat.completions.create(
            model=_get_env("MODEL_NAME", MODEL_NAME),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°åª’ä½“ç¼–è¾‘ï¼Œæ“…é•¿å…¬ä¼—å·æ—©æŠ¥æ’ç‰ˆä¸ä¿¡æ¯å¯†åº¦æ§åˆ¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        result = response.choices[0].message.content
        return result.strip()
    except Exception as e:
        print(f"Markdown æ’ç‰ˆå¤±è´¥: {e}")
        return None

def _safe_filename(value):
    value = (value or "").strip()
    if not value:
        return "untitled"
    value = re.sub(r'[<>:"/\\|?*\x00-\x1F]+', "_", value)
    value = re.sub(r"\s+", " ", value).strip()
    return value[:80] if len(value) > 80 else value

async def fetch_tophub_hot_text(url):
    source_auto = {"name": "Tophub Hot", "url": url, "type": "auto", "selector": "body"}
    content = await fetch_html_content(source_auto)
    if content and len(content) > 500:
        return content
    source_manual = {"name": "Tophub Hot (manual)", "url": url, "type": "manual_captcha", "selector": "body"}
    return await fetch_html_content(source_manual)

async def _extract_douyin_video_cards(page, max_items):
    return await page.evaluate(
        """
        (maxItems) => {
          const anchors = Array.from(document.querySelectorAll('a[href*="/video/"]'));
          const seen = new Set();
          const results = [];
          for (const a of anchors) {
            const raw = a.getAttribute('href') || '';
            const href = (a.href || raw || '').trim();
            if (!href) continue;
            let url = href;
            try {
              url = new URL(href, location.origin).toString();
            } catch (e) {}
            if (!url.includes('/video/')) continue;
            if (seen.has(url)) continue;
            const text = (a.innerText || '').replace(/\\s+/g, ' ').trim();
            const container = a.closest('div') || a.parentElement || a;
            const snippet = (container && container.innerText ? container.innerText : '').replace(/\\s+/g, ' ').trim();
            const title = (text || snippet || '').slice(0, 120);
            results.push({ url, title, snippet: snippet.slice(0, 240) });
            seen.add(url);
            if (results.length >= maxItems) break;
          }
          return results;
        }
        """,
        max_items,
    )

async def fetch_douyin_search_videos(keyword, max_items=3, manual=True):
    search_url = f"https://www.douyin.com/search/{urllib.parse.quote(keyword)}"
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            locale="zh-CN",
        )
        page = await context.new_page()
        try:
            await page.goto(search_url, timeout=60000)
            await page.wait_for_timeout(4000)
            try:
                await page.wait_for_load_state("networkidle", timeout=12000)
            except Exception:
                pass

            body_text = ""
            try:
                body_text = await page.inner_text("body")
            except Exception:
                body_text = ""
            body_text_l = body_text.lower()
            need_manual = (
                len(body_text) < 400
                or "éªŒè¯ç " in body_text
                or "å®‰å…¨éªŒè¯" in body_text
                or "verify" in body_text_l
                or "just a moment" in body_text_l
                or "captcha" in body_text_l
            )
            if manual and need_manual:
                print("\n" + "=" * 50)
                print("ã€éœ€è¦äººå·¥ä»‹å…¥ã€‘æŠ–éŸ³å¯èƒ½è§¦å‘éªŒè¯/ç™»å½•ã€‚")
                print(f"å·²æ‰“å¼€ï¼š{search_url}")
                print("=" * 50 + "\n")
                await asyncio.to_thread(input, ">> è¯·åœ¨æµè§ˆå™¨é‡Œå®ŒæˆéªŒè¯å¹¶è¿›å…¥æœç´¢ç»“æœé¡µåï¼ŒæŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")
                await page.wait_for_timeout(2000)

            cards = await _extract_douyin_video_cards(page, max_items=max_items)
            normalized = []
            for c in cards or []:
                url = (c.get("url") or "").strip()
                if not url:
                    continue
                normalized.append(
                    {
                        "url": url,
                        "title": (c.get("title") or "").strip(),
                        "snippet": (c.get("snippet") or "").strip(),
                    }
                )
            return normalized[:max_items]
        finally:
            await browser.close()

async def microtoutiao_analyze_with_llm(payload):
    client = _make_openai_client()
    if not client:
        print("é”™è¯¯ï¼šæœªå®‰è£… openai åº“ã€‚")
        return None

    prompt = f"""
ä½ æ˜¯ä¸€ä½çŸ­è§†é¢‘è¿è¥ä¸“å®¶ + ä»Šæ—¥å¤´æ¡å¾®å¤´æ¡å†™ä½œæ•™ç»ƒã€‚è¯·åŸºäºæˆ‘æä¾›çš„â€œTopHubçƒ­ç‚¹æ¦œå• + æŠ–éŸ³æœç´¢ç»“æœå¡ç‰‡ä¿¡æ¯â€ï¼Œåšé€‰é¢˜å†³ç­–ä¸å¯æ‰§è¡Œæ‹†è§£ã€‚

ç¡¬æ€§è¦æ±‚ï¼š
1) åªå…è®¸ä½¿ç”¨è¾“å…¥ä¸­å‡ºç°çš„ä¿¡æ¯ï¼Œä¸èƒ½å‡­ç©ºç¼–é€ å…·ä½“å‰§æƒ…ã€äººç‰©ç»å†ã€è§†é¢‘ç”»é¢ä¸éŸ³é¢‘ç»†èŠ‚ã€‚
2) å¦‚æœæŸé¡¹ä¿¡æ¯æ— æ³•ä»è¾“å…¥åˆ¤æ–­ï¼Œå¿…é¡»è¾“å‡ºâ€œæœªçŸ¥â€ï¼Œå¹¶ç»™å‡ºâ€œéœ€åœ¨çœ‹è¿‡è§†é¢‘åéªŒè¯çš„æ£€æŸ¥ç‚¹â€ã€‚
3) è¾“å‡ºå¿…é¡»æ˜¯çº¯ JSONï¼Œä¸èƒ½åŒ…å« Markdownã€è§£é‡Šæ–‡å­—ã€ä»£ç å—æ ‡è®°ã€‚

ç›®æ ‡ï¼š
- ä»å€™é€‰çƒ­ç‚¹é‡ŒæŒ‘ 1 ä¸ªæœ€é€‚åˆåšå¾®å¤´æ¡çš„æ ¸å¿ƒå…³é”®è¯ï¼›
- ä»å¯¹åº”çš„æŠ–éŸ³è§†é¢‘å¡ç‰‡é‡ŒæŒ‘ 1-2 æ¡ä½œä¸ºâ€œç´ æå…¥å£â€ï¼ˆå¦‚æœæ²¡æœ‰ä¹Ÿè¦è¯´æ˜åŸå› å¹¶ç»™å‡ºæ›¿ä»£ç­–ç•¥ï¼‰ï¼›
- äº§å‡ºï¼šå…³é”®è¯çŸ©é˜µã€æƒ…ç»ª/äº‰è®®ç‚¹ã€äºŒåˆ›å»ºè®®ã€åˆè§„é£é™©æç¤ºã€ä»¥åŠæ–‡ç« å†™ä½œè®¡åˆ’ï¼ˆæ ‡é¢˜å¤‡é€‰+äº”æ®µå¼è¦ç‚¹ï¼‰ã€‚

è¾“å‡º JSON Schemaï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆå­—æ®µåï¼‰ï¼š
{{
  "chosen_keyword": "string",
  "chosen_reason": "string",
  "chosen_videos": [
    {{"url":"string","title":"string","why":"string"}}
  ],
  "unknown_checkpoints": ["string"],
  "framework": {{
    "text_layer": "string",
    "visual_layer": "string",
    "audio_layer": "string",
    "interaction_layer": "string",
    "traffic_layer": "string"
  }},
  "burst_keywords": {{
    "sensitivity": "æ™®é€š|çƒ­é—¨|ç°è±¡çº§",
    "controversy": "æ™®é€š|çƒ­é—¨|ç°è±¡çº§",
    "virality": "æ™®é€š|çƒ­é—¨|ç°è±¡çº§",
    "phrases": ["string"],
    "templates": ["string"]
  }},
  "keyword_matrix": [
    {{"theme":"string","keywords":["string"]}}
  ],
  "comments_emotion": {{
    "likely_emotions": ["string"],
    "possible_conflicts": ["string"]
  }},
  "secondary_creation": {{
    "angles": ["string"],
    "cta_questions": ["string"]
  }},
  "compliance": {{
    "risk_points": ["string"],
    "safe_wording": ["string"]
  }},
  "article_plan": {{
    "audience": "string",
    "titles": ["string","string","string"],
    "outline": [
      {{"part":"hook","notes":"string"}},
      {{"part":"pain","notes":"string"}},
      {{"part":"reveal","notes":"string"}},
      {{"part":"climax","notes":"string"}},
      {{"part":"ending","notes":"string"}}
    ],
    "hashtags": ["string"]
  }}
}}

è¾“å…¥æ•°æ®ï¼š
{json.dumps(payload, ensure_ascii=False)}
"""

    print("æ­£åœ¨è¯·æ±‚ AI è¿›è¡Œé€‰é¢˜ä¸ç´ æåˆ†æ...")
    response = client.chat.completions.create(
        model=_get_env("MODEL_NAME", MODEL_NAME),
        messages=[
            {"role": "system", "content": "ä½ æ“…é•¿çƒ­ç‚¹é€‰é¢˜ã€çŸ­è§†é¢‘æ‹†è§£ä¸å¾®å¤´æ¡å†™ä½œã€‚"},
            {"role": "user", "content": prompt},
        ],
        stream=False,
    )
    raw = response.choices[0].message.content
    cleaned = _clean_json_text(raw)
    try:
        return json.loads(cleaned)
    except Exception as e:
        print(f"å¾®å¤´æ¡åˆ†æ JSON è§£æå¤±è´¥: {e}")
        return None

async def microtoutiao_write_with_llm(analysis_json):
    client = _make_openai_client()
    if not client:
        print("é”™è¯¯ï¼šæœªå®‰è£… openai åº“ã€‚")
        return None

    keyword = (analysis_json or {}).get("chosen_keyword") or ""
    prompt = f"""
ä½ æ˜¯ä¸€ä½æ‹¥æœ‰10å¹´ç»éªŒçš„æ–°åª’ä½“è¿è¥æ€»ç›‘ï¼ŒåŒæ—¶æ˜¯ä»Šæ—¥å¤´æ¡ç™¾ä¸‡ç²‰ä¸è´¦å·å¾¡ç”¨å†™æ‰‹ã€‚è¯·æ ¹æ®æˆ‘æä¾›çš„â€œé€‰é¢˜åˆ†æ JSONâ€ï¼Œå†™ä¸€ç¯‡å¯ç›´æ¥å‘å¸ƒçš„å¾®å¤´æ¡é•¿æ–‡ã€‚

ç¡¬æ€§è¦æ±‚ï¼š
1) åªå…è®¸åŸºäº analysis_json ä¸­ç»™å‡ºçš„äº‹å®/åˆ¤æ–­å†™ä½œï¼›ä¸èƒ½ç¼–é€ å…·ä½“äººç‰©å§“åã€æœºæ„å†…å¹•ã€ç²¾ç¡®æ•°æ®ä¸æ—¶é—´ç‚¹ã€‚
2) å…è®¸ä½¿ç”¨â€œæŸç ”ç©¶é™¢/ä¸šå†…äººå£«/ä¸å°‘äºº/å¾ˆå¤šå®¶åº­â€ç­‰æ¨¡ç³ŠåŒ–è¡¨è¾¾ï¼Œä½†å¿…é¡»é¿å…ç»å¯¹åŒ–å£å»ï¼ˆä¾‹å¦‚â€œç ”ç©¶è¡¨æ˜â€â€œä¸“å®¶æŒ‡å‡ºâ€ï¼‰ã€‚
3) ç¦æ­¢å‡ºç°â€œä½œä¸ºä¸€åAIâ€ç›¸å…³è¡¨è¿°ã€‚
4) ç»“æ„ä½¿ç”¨é»„é‡‘äº”æ®µå¼ï¼šhook/pain/reveal/climax/endingã€‚
5) æ€»é•¿åº¦ 1800-2500 å­—ã€‚
6) æ ‡é¢˜ä¸‰é€‰ä¸€ï¼šä» analysis_json.article_plan.titles ä¸­é€‰ 1 ä¸ªåšæœ€ç»ˆæ ‡é¢˜ï¼Œå¹¶ä¿è¯æ ‡é¢˜å‰ 10 ä¸ªå­—åŒ…å«æ ¸å¿ƒå…³é”®è¯â€œ{keyword}â€ï¼ˆè‹¥ä¸æ»¡è¶³è¯·å¾®è°ƒä½†ä¸æ”¹å˜å«ä¹‰ï¼‰ã€‚
7) ç»“å°¾å¿…é¡»ç”¨é—®å¥å¼•å¯¼è¯„è®ºï¼›å¹¶åœ¨æ–‡æœ«è¾“å‡º 3-6 ä¸ª hashtagsï¼ˆä» analysis_json.article_plan.hashtags é€‰ï¼Œå¿…è¦æ—¶å¯å°‘é‡å¾®è°ƒï¼‰ã€‚
8) åªè¾“å‡º Markdownï¼šç¬¬ä¸€è¡Œæ˜¯æ ‡é¢˜ï¼ˆ# æ ‡é¢˜ï¼‰ï¼Œæ­£æ–‡ä¸ºæ™®é€šæ®µè½ï¼›ä¸è¦è¾“å‡ºä»»ä½•é¢å¤–è¯´æ˜ã€‚

analysis_jsonï¼š
{json.dumps(analysis_json, ensure_ascii=False)}
"""

    print("æ­£åœ¨è¯·æ±‚ AI ç”Ÿæˆå¾®å¤´æ¡æ–‡ç« ...")
    response = client.chat.completions.create(
        model=_get_env("MODEL_NAME", MODEL_NAME),
        messages=[
            {"role": "system", "content": "ä½ æ“…é•¿æŠŠçƒ­ç‚¹å†™æˆé«˜è½¬å‘å¾®å¤´æ¡ã€‚"},
            {"role": "user", "content": prompt},
        ],
        stream=False,
    )
    return (response.choices[0].message.content or "").strip()

async def run_microtoutiao(args):
    tophub_text = await fetch_tophub_hot_text(args.tophub_url)
    if not tophub_text:
        print("TopHub çƒ­æ¦œæŠ“å–å¤±è´¥ã€‚")
        return

    hot_items = build_hot_items(tophub_text, max_items=max(args.max_hot_items, args.candidate_keywords))
    if not hot_items:
        print("TopHub çƒ­æ¦œè§£æå¤±è´¥ã€‚")
        return

    hot_items = hot_items[: args.max_hot_items]
    candidate = [it["title"] for it in hot_items[: args.candidate_keywords] if it.get("title")]
    candidate = [c.strip() for c in candidate if c and len(c.strip()) >= 2]
    if not candidate:
        print("æœªèƒ½ä»çƒ­æ¦œä¸­æå–å€™é€‰å…³é”®è¯ã€‚")
        return

    douyin_map = {}
    for kw in candidate:
        print(f"æŠ–éŸ³æœç´¢ï¼š{kw}")
        try:
            videos = await fetch_douyin_search_videos(kw, max_items=args.videos_per_keyword, manual=args.manual_douyin)
        except Exception as e:
            print(f"æŠ–éŸ³æœç´¢å¤±è´¥ï¼š{kw}ï¼Œé”™è¯¯: {e}")
            videos = []
        douyin_map[kw] = videos

    payload = {
        "date": datetime.datetime.now().strftime("%Y-%m-%d"),
        "tophub_url": args.tophub_url,
        "hot_items": hot_items,
        "douyin_search_results": douyin_map,
        "constraints": {"audience": "30-55å²ä¸‰å››çº¿åŸå¸‚ç”¨æˆ·ä¸ºä¸»"},
    }
    analysis = await microtoutiao_analyze_with_llm(payload)
    if not analysis:
        print("å¾®å¤´æ¡åˆ†æå¤±è´¥ã€‚")
        return

    article_md = await microtoutiao_write_with_llm(analysis)
    if not article_md:
        print("å¾®å¤´æ¡ç”Ÿæˆå¤±è´¥ã€‚")
        return

    out_dir = args.out_dir
    os.makedirs(out_dir, exist_ok=True)
    date_str = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    chosen_kw = _safe_filename(analysis.get("chosen_keyword") or "çƒ­ç‚¹")
    out_path = os.path.join(out_dir, f"å¾®å¤´æ¡_{date_str}_{chosen_kw}.md")
    with open(out_path, "w", encoding="utf-8") as f:
        f.write(article_md.strip() + "\n")
    print(f"å¾®å¤´æ¡å·²ç”Ÿæˆ: {out_path}")

def update_data_json(new_data):
    """
    æ›´æ–° data.json æ–‡ä»¶
    """
    try:
        with open('data.json', 'r', encoding='utf-8') as f:
            current_data = json.load(f)
        
        current_data['news_items'] = new_data.get('news_items', [])
        if isinstance(new_data.get("page_title"), str) and new_data["page_title"].strip():
            current_data["page_title"] = new_data["page_title"].strip()
        if isinstance(new_data.get("opening"), str):
            current_data["opening"] = new_data["opening"].strip()
        if isinstance(new_data.get("ending"), str):
            current_data["ending"] = new_data["ending"].strip()
        new_quote = new_data.get('quote')
        if isinstance(new_quote, str):
            current_data['quote']['text'] = new_quote
        elif isinstance(new_quote, dict):
             current_data['quote'] = new_quote
        if isinstance(new_data.get("markdown"), str):
            current_data["markdown"] = new_data["markdown"].strip()

        today = datetime.datetime.now() + datetime.timedelta(days=1)
        week_list = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        current_data['date_info']['date_str'] = today.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_data['date_info']['week_str'] = week_list[today.weekday()]
        
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=4)
            
        return current_data
    except Exception as e:
        print(f"æ›´æ–° data.json å¤±è´¥: {e}")
        return None

async def main():
    # 1. å°è¯•ä»ä¸åŒæºè·å–å†…å®¹
    content = None
    for source in SOURCES:
        content = await fetch_html_content(source)
        if content and len(content) > 500: # ç¡®ä¿è·å–åˆ°äº†è¶³å¤Ÿçš„å†…å®¹
            break
        print(f"[{source['name']}] è·å–å†…å®¹è¿‡å°‘æˆ–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæº...")
    
    if not content:
        print("æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥ã€‚")
        return

    # 2. å¤„ç†å†…å®¹
    data_processed = await process_with_llm(content)
    if not data_processed:
        print("AI å¤„ç†å¤±è´¥ã€‚")
        return
    markdown = await format_with_llm(data_processed)
    if markdown:
        data_processed["markdown"] = markdown

    # æ›´æ–° JSON
    try:
        current_data = update_data_json(data_processed)
        if current_data:
            print("data.json å·²æ›´æ–°ã€‚")
            
            # === æ–°å¢ï¼šç”Ÿæˆæ¯æ—¥æ–‡ç«  ===
            try:
                data = current_data
                date_str = data['date_info']['date_str']
                # æ›¿æ¢æ—¥æœŸä¸­çš„ä¸­æ–‡å­—ç¬¦ä»¥ç”¨äºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œè¿™é‡Œç›´æ¥ç”¨ï¼‰
                filename = f"{date_str}.md"
                article_dir = r"d:\zixun\æ¯æ—¥æ–‡ç« "
                if not os.path.exists(article_dir):
                    os.makedirs(article_dir)
                
                article_path = os.path.join(article_dir, filename)
                
                article_content = (data.get("markdown") or "").strip()
                if not article_content:
                    page_title = data.get("page_title") or f"{date_str} ä»Šæ—¥é€Ÿè§ˆ"
                    article_content = f"# {page_title}\n"
                    opening = (data.get("opening") or "").strip()
                    if opening:
                        article_content += f"> {opening}\n"
                    total_items = len(data['news_items'])
                    for i, item in enumerate(data['news_items'], 1):
                        article_content += f"## ğŸ”¸ {i}. {item['title']}\n"
                        title = (item.get("title") or "").strip()
                        keyword = title[:4] if len(title) >= 2 else "è¦é—»"
                        article_content += f"#{keyword}\n"
                        summary_long = (item.get("summary_long") or "").strip()
                        if summary_long:
                            article_content += f"### {summary_long}\n"
                            if i < total_items:
                                article_content += "\n"
                    ending = (data.get("ending") or "").strip()
                    if ending:
                        article_content += f"---\n{ending}\n"
                    quote_text = (data.get("quote") or {}).get("text") or ""
                    if quote_text:
                        article_content += f"{quote_text}\n"
                
                with open(article_path, 'w', encoding='utf-8') as f:
                    f.write(article_content)
                    
                print(f"æ–‡ç« å·²ç”Ÿæˆ: {article_path}")
            except Exception as e:
                print(f"ç”Ÿæˆæ–‡ç« å¤±è´¥: {e}")
            # ==========================
            
            print("æ­£åœ¨è°ƒç”¨ gen_image.py ç”Ÿæˆå›¾ç‰‡...")
            import gen_image
            await gen_image.main()
            
    except Exception as e:
        print(f"å¤„ç†æµç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(add_help=True)
    subparsers = parser.add_subparsers(dest="cmd")
    subparsers.required = False

    p_daily = subparsers.add_parser("daily")

    p_micro = subparsers.add_parser("micro")
    p_micro.add_argument("--tophub-url", default="https://tophub.today/n/Dgey31RvZq")
    p_micro.add_argument("--max-hot-items", type=int, default=30)
    p_micro.add_argument("--candidate-keywords", type=int, default=6)
    p_micro.add_argument("--videos-per-keyword", type=int, default=3)
    p_micro.add_argument("--no-manual-douyin", dest="manual_douyin", action="store_false", default=True)
    p_micro.add_argument("--out-dir", default=r"d:\zixun\æ¯æ—¥æ–‡ç« ")

    args = parser.parse_args()

    if args.cmd is None or args.cmd == "daily":
        asyncio.run(main())
        sys.exit(0)

    if args.cmd == "micro":
        asyncio.run(run_microtoutiao(args))
        sys.exit(0)

    parser.print_help()
