import asyncio
import json
import os
import re
import datetime
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# =================é…ç½®åŒºåŸŸ=================
# è¯·åœ¨è¿™é‡Œå¡«å…¥ä½ çš„ LLM API Key
API_KEY = "ms-b8df244e-aa5e-4392-b3bf-4b0e0f80c052" 
API_BASE_URL = "https://api-inference.modelscope.cn/v1" 
MODEL_NAME = "ZhipuAI/GLM-4.7" 

# æ•°æ®æºé…ç½®
SOURCES = [
    {
        "name": "Tophub Hot",
        "url": "https://tophub.today/hot",
        "type": "auto",
        "selector": "body"
    }
]
# =========================================

def build_hot_digest(text, max_items=120):
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    items = []
    i = 0
    while i < len(lines):
        if re.fullmatch(r"\d{1,3}", lines[i]):
            try:
                rank = int(lines[i])
            except Exception:
                i += 1
                continue
            title = lines[i + 1] if i + 1 < len(lines) else ""
            j = i + 2
            source = ""
            heat = ""
            if j < len(lines) and ("Â·" in lines[j] or "â€§" in lines[j]):
                source = lines[j].replace("Â·", "").replace("â€§", "").strip()
                j += 1
            if j < len(lines) and ("çƒ­åº¦" in lines[j] or re.search(r"\d", lines[j])):
                heat = lines[j].strip()
                j += 1
            if title and not re.fullmatch(r"\d{1,3}", title):
                items.append({"rank": rank, "title": title, "source": source, "heat": heat})
            i = j
        else:
            i += 1

    if not items:
        candidate_lines = []
        for line in lines[:1000]:
            if len(line) < 6:
                continue
            if "çƒ­åº¦" in line:
                continue
            candidate_lines.append(line)
        items = [{"rank": idx + 1, "title": t, "source": "", "heat": ""} for idx, t in enumerate(candidate_lines[:max_items])]

    digest_lines = []
    for item in items[:max_items]:
        meta = []
        if item.get("source"):
            meta.append(item["source"])
        if item.get("heat") and "çƒ­åº¦" in item["heat"]:
            meta.append(item["heat"])
        meta_str = f"ï¼ˆ{'ï¼Œ'.join(meta)}ï¼‰" if meta else ""
        digest_lines.append(f"{item['rank']}. {item['title']}{meta_str}")

    return "\n".join(digest_lines)

async def fetch_html_content(source):
    """
    é€šç”¨ HTML è·å–å™¨
    """
    print(f"æ­£åœ¨å°è¯•ä» [{source['name']}] è·å–å†…å®¹...")
    
    async with async_playwright() as p:
        # ç™¾åº¦çƒ­æœé€šå¸¸ä¸éœ€è¦å¤æ‚çš„éªŒè¯ï¼Œheadless=True å³å¯
        # Tophub éœ€è¦ headless=False
        headless = source['type'] != 'manual_captcha'
        
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            await page.goto(source['url'], timeout=60000)
            
            if source['type'] == 'manual_captcha':
                print("\n" + "="*50)
                print("ã€éœ€è¦æ‰‹åŠ¨éªŒè¯ã€‘")
                print("æµè§ˆå™¨å·²æ‰“å¼€ã€‚è¯·åœ¨å¼¹å‡ºçš„æµè§ˆå™¨çª—å£ä¸­å®ŒæˆéªŒè¯ï¼Œç›´åˆ°çœ‹åˆ°æ–°é—»åˆ—è¡¨ã€‚")
                print("="*50 + "\n")
                await asyncio.to_thread(input, ">> ç¡®è®¤é¡µé¢å·²åŠ è½½å®Œæ¯•ï¼Ÿè¯·æŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­ç¨‹åº...")
                
                # === Tophub ç‰¹æ®Šé€»è¾‘ï¼šå°è¯•è·å–æ™šæŠ¥ ===
                if "tophub.today" in source['url']:
                    print("å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰ã€æ™šæŠ¥ã€‘å†…å®¹...")
                    try:
                        # 1. è·å–å½“å‰é¡µé¢æ–‡æœ¬ï¼ˆé»˜è®¤æ—©æŠ¥ï¼‰
                        content_early = await page.evaluate("document.body.innerText")
                        
                        # 2. å°è¯•å¯»æ‰¾å¹¶ç‚¹å‡»â€œæ™šæŠ¥â€æŒ‰é’®
                        # å‡è®¾æŒ‰é’®åŒ…å«æ–‡æœ¬â€œæ™šæŠ¥â€
                        evening_btn = page.locator("text=æ™šæŠ¥").first
                        if await evening_btn.is_visible():
                            print("å‘ç°ã€æ™šæŠ¥ã€‘æŒ‰é’®ï¼Œå°è¯•åˆ‡æ¢...")
                            await evening_btn.click()
                            print("å·²ç‚¹å‡»ã€æ™šæŠ¥ã€‘ï¼Œç­‰å¾… 5 ç§’åŠ è½½...")
                            await page.wait_for_timeout(5000) # ç­‰å¾…å±€éƒ¨åˆ·æ–°ï¼Œå¢åŠ å»¶è¿Ÿé˜²æ­¢ç½‘é€Ÿæ…¢
                            
                            content_late = await page.evaluate("document.body.innerText")
                            
                            if content_late != content_early:
                                print("æˆåŠŸè·å–ã€æ™šæŠ¥ã€‘å†…å®¹ã€‚æ­£åœ¨åˆå¹¶æ—©æŠ¥ä¸æ™šæŠ¥...")
                                # å°†ä¸¤éƒ¨åˆ†å†…å®¹æ‹¼æ¥ï¼Œç”¨æ˜æ˜¾çš„åˆ†éš”ç¬¦
                                final_content = f"=== æ—©æŠ¥å†…å®¹ ===\n{content_early}\n\n=== æ™šæŠ¥å†…å®¹ ===\n{content_late}"
                                return final_content
                            else:
                                print("å†…å®¹æœªå˜åŒ–ï¼ˆå¯èƒ½å·²ç»æ˜¯æ™šæŠ¥æˆ–æ•°æ®æœªæ›´æ–°ï¼‰ã€‚")
                                return content_early
                        else:
                            print("æœªæ‰¾åˆ°ã€æ™šæŠ¥ã€‘åˆ‡æ¢æŒ‰é’®ï¼Œä½¿ç”¨å½“å‰é¡µé¢å†…å®¹ã€‚")
                            return content_early
                            
                    except Exception as e:
                        print(f"å°è¯•åˆ‡æ¢æ™šæŠ¥æ—¶å‡ºé”™: {e}ã€‚å°†ä»…ä½¿ç”¨å½“å‰å†…å®¹ã€‚")
                        # å‡ºé”™æ—¶å›é€€åˆ°è·å–å½“å‰å†…å®¹
                        pass
                # ========================================

            else:
                # è‡ªåŠ¨ç­‰å¾…
                print("ç­‰å¾…é¡µé¢åŠ è½½...")
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(3000)

            # è·å– HTML
            content = await page.content()
            
            # ä½¿ç”¨ BeautifulSoup æå–ä¸»è¦å†…å®¹ï¼Œå‡å°‘ token
            soup = BeautifulSoup(content, 'html.parser')
            
            # é’ˆå¯¹ä¸åŒæºçš„ç®€å•æ¸…ç†
            if "baidu" in source['url']:
                # ç™¾åº¦çƒ­æœçš„ä¸»è¦å†…å®¹åœ¨ main æˆ–ç‰¹å®š class ä¸­
                main_content = soup.find('div', class_='container') or soup.body
            else:
                main_content = soup.body
                
            # è½¬ä¸ºæ–‡æœ¬ (ä¿ç•™ä¸€å®šçš„ HTML ç»“æ„å¯èƒ½æ›´å¥½ï¼Œä½†çº¯æ–‡æœ¬æ›´çœ token)
            # è¿™é‡Œæˆ‘ä»¬ä¸ºäº†è®© LLM æ›´å¥½ç†è§£ç»“æ„ï¼Œæå–çº¯æ–‡æœ¬ä½†ä¿ç•™æ¢è¡Œ
            text_content = main_content.get_text(separator='\n', strip=True)
            
            print(f"æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(text_content)} å­—ç¬¦")
            return text_content
            
        except Exception as e:
            print(f"è·å–å¤±è´¥: {e}")
            return None
        finally:
            await browser.close()

async def process_with_llm(text):
    """
    è°ƒç”¨ LLM API å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æå’Œæ€»ç»“ (ä»¿ç…§ 60s-static-host æ€è·¯)
    """
    try:
        from openai import OpenAI
    except ImportError:
        print("é”™è¯¯ï¼šæœªå®‰è£… openai åº“ã€‚")
        return None

    client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)
    
    prompt = f"""
ä½ ç°åœ¨æ˜¯å…¬ä¼—å·ã€Œæ—©çŸ¥é“å¹²è´§é“ºã€çš„æ™šé—´å¨±ä¹çƒ­ç‚¹ç¼–è¾‘ï¼Œè´Ÿè´£ç­›é€‰å¹¶åŠ å·¥ä»Šæ—¥çƒ­æ¦œâ€œæ¦œä¸­æ¦œâ€åŸå§‹æ¡ç›®ã€‚
ã€ç­›é€‰è§„åˆ™ã€‘
1.  åªä¿ç•™ **æ³›å¨±ä¹ã€æ‚è°ˆåŒ–ã€è½»æ¾å‘** å†…å®¹ï¼ŒåŒ…å«ä½†ä¸é™äºï¼šæ˜æ˜Ÿå…«å¦ã€å½±è§†ç»¼åŠ¨æ€ã€ä½“è‚²å¨±ä¹äº‹ä»¶ã€ç½‘ç»œçƒ­æ¢—ã€è½»ç¤¾ä¼šè¶£å‘³è¯é¢˜ï¼ˆå¦‚æ ¡å›­è¶£äº‹ã€èŒå® èŒåœºæ¢—ã€å¥‡è‘©æ¶ˆè´¹ç°è±¡ï¼‰ï¼›
2.  ä¸¥æ ¼å‰”é™¤ä»¥ä¸‹å†…å®¹ï¼šä¸¥è‚ƒæ”¿æ²»å†›äº‹ã€å®è§‚ç»æµåˆ†æã€ç¾éš¾äº‹æ•…ã€å›½é™…å†²çªã€è´Ÿé¢ç¤¾ä¼šæ¡ˆä»¶ã€åŒ»ç–—å¥åº·è­¦ç¤ºç±»å†…å®¹ã€‚

ã€æ ¼å¼ä¸å­—æ®µå¼ºåˆ¶è¦æ±‚ã€‘
1.  ç”Ÿæˆ 1 ä¸ªå…¬ä¼—å·ç¡å‰é˜…è¯»ä¸“ç”¨æ ‡é¢˜å­—æ®µ "page_title"ï¼š
    - ç»“æ„ï¼š2ä¸ªé«˜è¯é¢˜åº¦å…³é”®è¯ + æ™šé—´/ç¡å‰/è¶£é—»å±æ€§è¯ + å¥½å¥‡å¿ƒé’©å­
    - å­—æ•°ï¼š20-30å­—ï¼Œæ‰‹æœºç«¯æ˜¾ç¤ºå®Œæ•´ï¼Œç¦æ­¢è¶…è¿‡30å­—
2.  ç”Ÿæˆ 1 ä¸ªå¼€å¤´æ–‡æ¡ˆå­—æ®µ "opening"ï¼š
    - é£æ ¼ï¼šå£è¯­åŒ–ç½‘æ„Ÿï¼Œåƒå’Œæœ‹å‹èŠå¤©ï¼Œ30-50å­—
    - å†…å®¹ï¼šç”¨1-2ä¸ªçˆ†ç‚¹çƒ­ç‚¹é’©å­+é˜…è¯»å¼•å¯¼ï¼Œç‚¹æ˜â€œç¡å‰é€Ÿè§ˆã€æ— å‹åŠ›â€çš„å±æ€§
3.  ç”Ÿæˆ 1 ä¸ªç»“å°¾æ–‡æ¡ˆå­—æ®µ "ending"ï¼š
    - é£æ ¼ï¼šè½»æ¾äº’åŠ¨å‹ï¼Œ40-60å­—
    - å†…å®¹ï¼šåŒ…å«äº’åŠ¨æé—®+æ¬¡æ—¥å†…å®¹é¢„å‘Š+æ˜Ÿæ ‡å¼•å¯¼ï¼Œé€‚é…å…¬ä¼—å·è¿è¥éœ€æ±‚
    - ç‚¹ç¼€ï¼šå¯åŠ 1ä¸ªå¼•å¯¼ç±»emojiï¼ˆå¦‚â­/â°ï¼‰ï¼Œçªå‡ºé‡ç‚¹
4.  ä»å€™é€‰æ¡ç›®ç­›é€‰ **12-15æ¡**ï¼ˆä¼˜å…ˆå‡‘15æ¡ï¼Œä¸è¶³åˆ™æŒ‰å®é™…æ•°é‡è¾“å‡ºï¼‰ï¼Œæ¯æ¡å¿…é¡»åŒ…å«3ä¸ªå­—æ®µï¼Œå­—æ®µå¡«å†™ä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„èŒƒï¼š
    - "title"ï¼šç²¾ç‚¼çŸ­å¥æ ‡é¢˜ï¼Œâ‰¤22å­—ï¼Œä¸ç”¨æé—®å¥å¼ï¼Œç›´æ¥é™ˆè¿°æ ¸å¿ƒäº‹ä»¶ï¼Œçªå‡ºçˆ†ç‚¹ï¼›å¯åŠ 1ä¸ªåˆ†ç±»ç±»emojiï¼ˆå¦‚ğŸ‘‘æ˜æ˜Ÿ/ğŸ¬å½±è§†/ğŸ¶èŒå® /ğŸœæ¶ˆè´¹ï¼‰æ ‡æ³¨ç±»å‹
    - "summary_short"ï¼šä¸€å¥è¯çœ‹ç‚¹ï¼Œ20-30å­—ï¼Œå¿…é¡»æ‹†æˆ2ä¸ªçŸ­åˆ†å¥ç”¨ã€Œï¼›ã€è¿æ¥ï¼Œ**ä¸¥ç¦å¤è¿°titleå†…å®¹**ï¼Œåªæç‚¼ç”¨æˆ·å…³å¿ƒçš„â€œçˆ½ç‚¹/ç¬‘ç‚¹/äº‰è®®ç‚¹â€ï¼›ç¦æ­¢åŠ emoji
    - "summary_long"ï¼šèµ„æ·±å¨±ä¹è®°è€…å£å»çš„äº‹ä»¶è®²è§£+ç‚¹è¯„ï¼Œ90-120å­—ï¼Œä¿¡æ¯å¯†åº¦é«˜ï¼Œå¿…é¡»è¯´æ¸…ã€Œäº‹ä»¶ä¸ºä»€ä¹ˆç«/æ ¸å¿ƒçœ‹ç‚¹åœ¨å“ª/èƒŒåçš„è¶£å‘³é€»è¾‘ã€ï¼Œè¯­è¨€è½»æ¾åæ§½ï¼Œä¸å †ç Œä¸“ä¸šæœ¯è¯­ï¼›ç¦æ­¢åŠ emoji
5.  æå– 1 å¥ã€Œåƒç“œå“²å­¦ã€é‡‘å¥ä½œä¸º "tip"ï¼šä¸å¸¦ä½œè€…ï¼Œå£è¯­åŒ–æ¥åœ°æ°”ï¼Œé€‚é…ç¡å‰åƒç“œæ°›å›´ï¼Œæœ‰å…±é¸£æ„Ÿï¼›ç¦æ­¢å†™é¸¡æ±¤å¼å¤§é“ç†ï¼Œç¦æ­¢åŠ emoji

ã€è¾“å‡ºè¦æ±‚ã€‘
- åªè¿”å›çº¯ JSON æ ¼å¼ï¼Œæ— ä»»ä½• Markdown æ ‡è®°ã€æ— å¤šä½™è§£é‡Šæ–‡å­—
- è¯­è¨€é£æ ¼ç»Ÿä¸€ï¼šæ¥åœ°æ°”ã€å¸¦ç½‘æ„Ÿï¼Œç¬¦åˆç¡å‰æ”¾æ¾é˜…è¯»åœºæ™¯ï¼Œé¿å…ç”Ÿç¡¬ä¹¦é¢è¯­
- emojiæ€»é‡é™åˆ¶ï¼šå…¨æ–‡ä¸è¶…è¿‡18ä¸ªï¼Œé¿å…æ‚ä¹±

ã€å†…å®¹ã€‘
{text[:15000]}
"""

    print("æ­£åœ¨è¯·æ±‚ AI è¿›è¡Œæ™ºèƒ½è§£æ...")
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å¨±ä¹æ–°é—»å·¥ä½œè€…ï¼Œæ“…é•¿ç­›é€‰çƒ­æ¦œé‡Œçš„æ³›å¨±ä¹è¯é¢˜å¹¶åšä¸“ä¸šè§£è¯»ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        
        result = response.choices[0].message.content
        result = result.replace("```json", "").replace("```", "").strip()
        
        # é’ˆå¯¹æ€§ä¿®å¤ï¼šåªæœ‰å½“ä¸­æ–‡å¼•å·å‡ºç°åœ¨é”®å€¼å¯¹çš„åˆ†éš”ç¬¦ä½ç½®æ—¶æ‰æ›¿æ¢
        import re
        # æ›¿æ¢é”®å€¼å¯¹å†’å·åçš„å¼€å¼•å·: : â€œ -> : "
        result = re.sub(r':\s*â€œ', ': "', result)
        # æ›¿æ¢é€—å·å‰çš„é—­å¼•å·: â€, -> ",
        result = re.sub(r'â€\s*,', '",', result)
        # æ›¿æ¢å¯¹è±¡ç»“æŸå‰çš„é—­å¼•å·: â€} -> "}
        result = re.sub(r'â€\s*}', '"}', result)
        # æ›¿æ¢åˆ—è¡¨ç»“æŸå‰çš„é—­å¼•å·: â€] -> "]
        result = re.sub(r'â€\s*]', '"]', result)
        
        # print(f"DEBUG: LLM åŸå§‹å“åº”:\n{result}\n")
        
        data = json.loads(result)

        if not isinstance(data, dict):
            data = {"news_items": []}

        if isinstance(data.get("page_title"), dict):
            data["page_title"] = data["page_title"].get("text") or data["page_title"].get("title") or ""
        if "page_title" not in data:
            data["page_title"] = data.get("headline") or data.get("title_text") or ""
        if not isinstance(data.get("page_title"), str):
            data["page_title"] = ""
        data["page_title"] = data["page_title"].strip()
        if len(data["page_title"]) > 30:
            data["page_title"] = data["page_title"][:30]

        if isinstance(data.get("opening"), dict):
            data["opening"] = data["opening"].get("text") or data["opening"].get("content") or ""
        if "opening" not in data:
            data["opening"] = data.get("intro") or data.get("lead") or ""
        if not isinstance(data.get("opening"), str):
            data["opening"] = ""
        data["opening"] = data["opening"].strip()

        if isinstance(data.get("ending"), dict):
            data["ending"] = data["ending"].get("text") or data["ending"].get("content") or ""
        if "ending" not in data:
            data["ending"] = data.get("outro") or data.get("closing") or ""
        if not isinstance(data.get("ending"), str):
            data["ending"] = ""
        data["ending"] = data["ending"].strip()

        if ('news_items' not in data) or (not isinstance(data.get('news_items'), list)) or (len(data.get('news_items') or []) == 0):
            list_candidates = []
            for k, v in data.items():
                if isinstance(v, list) and len(v) > 0:
                    list_candidates.append((k, v))
            if list_candidates:
                list_candidates.sort(key=lambda x: len(x[1]), reverse=True)
                data['news_items'] = list_candidates[0][1]
        
        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœè¿”å›äº† news è€Œä¸æ˜¯ news_items
        if 'news' in data and 'news_items' not in data:
            data['news_items'] = data['news']
            
        # æ ‡å‡†åŒ–ï¼šç¡®ä¿ news_items æ˜¯å¯¹è±¡åˆ—è¡¨
        if 'news_items' in data and isinstance(data['news_items'], list):
            normalized_items = []
            for item in data['news_items']:
                if isinstance(item, str):
                    # æ—§æ ¼å¼å…¼å®¹ï¼ˆè™½ç„¶ Prompt è¦æ±‚äº†æ–°æ ¼å¼ï¼Œé˜²ä¸‡ä¸€ï¼‰
                    parts = item.split('ï½œ')
                    title = parts[0]
                    summary = parts[1] if len(parts) > 1 else ""
                    normalized_items.append({
                        "title": title,
                        "summary_short": summary[:30], # æˆªæ–­ä½œä¸ºçŸ­æ‘˜è¦
                        "summary_long": summary # ä½œä¸ºé•¿æ‘˜è¦
                    })
                elif isinstance(item, dict):
                    # ç¡®ä¿å­—æ®µé½å…¨
                    if "title" not in item:
                        item["title"] = item.get("name") or item.get("topic") or item.get("headline") or ""
                    if "summary_short" not in item:
                        item["summary_short"] = item.get("summary") or item.get("brief") or item.get("one_liner") or ""
                        item["summary_short"] = item["summary_short"][:30]
                    if "summary_long" not in item:
                        item["summary_long"] = item.get("analysis") or item.get("detail") or item.get("comment") or item.get("summary") or ""
                    if item.get("title"):
                        normalized_items.append(item)
            data['news_items'] = normalized_items
        
        # å…¼å®¹æ€§å¤„ç†ï¼šå°† tip æ˜ å°„ä¸º quote
        if 'tip' in data and 'quote' not in data:
            data['quote'] = {"text": data['tip'], "author": ""}

        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœ quote æ˜¯å­—ç¬¦ä¸²è€Œä¸æ˜¯å¯¹è±¡
        if isinstance(data.get('quote'), str):
            data['quote'] = {"text": data['quote'], "author": ""}
        
        return data
    except json.JSONDecodeError:
        print(f"JSON è§£æå¤±è´¥ã€‚åŸå§‹å“åº”: {result}")
        # å°è¯•ä½¿ç”¨æ­£åˆ™æå–ç»“æ„åŒ–æ•°æ®ï¼ˆå…œåº•ï¼‰
        try:
            import re
            # å°è¯•åŒ¹é…å®Œæ•´çš„ item ç»“æ„
            item_pattern = r'"title":\s*"(.*?)".*?"summary_short":\s*"(.*?)".*?"summary_long":\s*"(.*?)"'
            matches = re.findall(item_pattern, result, re.S)
            
            if matches:
                print(f"é€šè¿‡æ­£åˆ™æ‰¾å› {len(matches)} æ¡ç»“æ„åŒ–æ–°é—»")
                news_items = []
                for m in matches:
                    news_items.append({
                        "title": m[0],
                        "summary_short": m[1],
                        "summary_long": m[2]
                    })
                
                # å°è¯•æå– tip
                tip_match = re.search(r'"tip":\s*"(.*?)"', result)
                tip_text = tip_match.group(1) if tip_match else "ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ·ã€‚"
                
                return {
                    "news_items": news_items,
                    "quote": {"text": tip_text, "author": ""}
                }
            
            # å¦‚æœç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²å¹¶å°½åŠ›æ‹¼å‡‘
            print("æ­£åˆ™ç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²...")
            all_strings = re.findall(r'"([^"]+)"', result)
            if all_strings and len(all_strings) > 5:
                 # å‡è®¾å‰å‡ ä¸ªæ˜¯ keysï¼Œæˆ‘ä»¬å¾ˆéš¾å‡†ç¡®æ¢å¤ï¼Œè¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨é¿å…å´©æºƒ
                 return {
                    "news_items": [{"title": "æ•°æ®è§£æé”™è¯¯", "summary_short": "è¯·æ£€æŸ¥æ—¥å¿—", "summary_long": f"åŸå§‹æ•°æ®ç‰‡æ®µ: {all_strings[:3]}..."}],
                    "quote": {"text": "ç³»ç»Ÿæ•…éšœ", "author": "Error"}
                 }
        except Exception as e:
            print(f"å…œåº•è§£æä¹Ÿå¤±è´¥: {e}")
            pass
        return None
    except Exception as e:
        print(f"AI åˆ†æå¤±è´¥: {e}")
        return None

def update_data_json(new_data):
    """
    æ›´æ–° data.json æ–‡ä»¶
    """
    try:
        base_dir = os.path.dirname(os.path.abspath(__file__))
        data_path = os.path.join(base_dir, 'data.json')
        with open(data_path, 'r', encoding='utf-8') as f:
            current_data = json.load(f)
        
        current_data['news_items'] = new_data.get('news_items', [])
        if isinstance(new_data.get("page_title"), str) and new_data["page_title"].strip():
            current_data["page_title"] = new_data["page_title"].strip()
        if isinstance(new_data.get("opening"), str):
            current_data["opening"] = new_data["opening"].strip()
        if isinstance(new_data.get("ending"), str):
            current_data["ending"] = new_data["ending"].strip()
        new_quote = new_data.get('quote')
        if isinstance(new_quote, str):
            current_data['quote']['text'] = new_quote
        elif isinstance(new_quote, dict):
             current_data['quote'] = new_quote

        today = datetime.datetime.now()
        week_list = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        current_data['date_info']['date_str'] = today.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_data['date_info']['week_str'] = week_list[today.weekday()]
        
        with open(data_path, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=4)
            
        return current_data
    except Exception as e:
        print(f"æ›´æ–° data.json å¤±è´¥: {e}")
        return None

async def main():
    # 1. å°è¯•ä»ä¸åŒæºè·å–å†…å®¹
    content = None
    for source in SOURCES:
        content = await fetch_html_content(source)
        if content and len(content) > 500: # ç¡®ä¿è·å–åˆ°äº†è¶³å¤Ÿçš„å†…å®¹
            break
        print(f"[{source['name']}] è·å–å†…å®¹è¿‡å°‘æˆ–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæº...")
    
    if not content:
        print("æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥ã€‚")
        return

    # 2. å¤„ç†å†…å®¹
    digest = build_hot_digest(content)
    data_processed = await process_with_llm(digest)
    if not data_processed:
        print("AI å¤„ç†å¤±è´¥ã€‚")
        return

    # æ›´æ–° JSON
    try:
        current_data = update_data_json(data_processed)
        if current_data:
            print("data.json å·²æ›´æ–°ã€‚")
            
            # === æ–°å¢ï¼šç”Ÿæˆæ¯æ—¥æ–‡ç«  ===
            try:
                data = current_data
                date_str = data['date_info']['date_str']
                # æ›¿æ¢æ—¥æœŸä¸­çš„ä¸­æ–‡å­—ç¬¦ä»¥ç”¨äºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œè¿™é‡Œç›´æ¥ç”¨ï¼‰
                filename = f"{date_str}.txt"
                base_dir = os.path.dirname(os.path.abspath(__file__))
                article_dir = os.path.join(base_dir, "æ¯æ—¥æ–‡ç« ")
                if not os.path.exists(article_dir):
                    os.makedirs(article_dir)
                
                article_path = os.path.join(article_dir, filename)
                
                page_title = data.get("page_title") or "60sæ™šé—´é—²è¯»"
                article_content = f"ã€{page_title}ã€‘\n\n"
                article_content += f"æ—¥æœŸï¼š{date_str}\n"
                article_content += f"ä»Šæ—¥é‡‘å¥ï¼š{data['quote']['text']}\n"
                if data['quote']['author']:
                    article_content += f"â€”â€” {data['quote']['author']}\n"
                article_content += "\n" + "="*30 + "\n\n"

                opening = (data.get("opening") or "").strip()
                if opening:
                    article_content += f"{opening}\n\n"
                
                for i, item in enumerate(data['news_items'], 1):
                    article_content += f"{i}. {item['title']}\n"
                    article_content += f"   {item['summary_long']}\n\n"

                ending = (data.get("ending") or "").strip()
                if ending:
                    article_content += f"{ending}\n"
                
                with open(article_path, 'w', encoding='utf-8') as f:
                    f.write(article_content)
                    
                print(f"æ–‡ç« å·²ç”Ÿæˆ: {article_path}")
            except Exception as e:
                print(f"ç”Ÿæˆæ–‡ç« å¤±è´¥: {e}")
            # ==========================
            
            print("æ­£åœ¨è°ƒç”¨ gen_image.py ç”Ÿæˆå›¾ç‰‡...")
            import gen_image
            await gen_image.main()
            
    except Exception as e:
        print(f"å¤„ç†æµç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    asyncio.run(main())
