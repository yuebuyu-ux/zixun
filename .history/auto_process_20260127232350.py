import asyncio
import json
import os
import re
import datetime
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup

# =================é…ç½®åŒºåŸŸ=================
# è¯·åœ¨è¿™é‡Œå¡«å…¥ä½ çš„ LLM API Key
API_KEY = "ms-b8df244e-aa5e-4392-b3bf-4b0e0f80c052" 
API_BASE_URL = "https://api-inference.modelscope.cn/v1" 
MODEL_NAME = "ZhipuAI/GLM-4.7" 

# æ•°æ®æºé…ç½®
SOURCES = [
    {
        "name": "Tophub Daily",
        "url": "https://tophub.today/daily",
        "type": "manual_captcha", # éœ€è¦æ‰‹åŠ¨éªŒè¯
        "selector": "body"
    }
]
# =========================================

async def fetch_html_content(source):
    """
    é€šç”¨ HTML è·å–å™¨
    """
    print(f"æ­£åœ¨å°è¯•ä» [{source['name']}] è·å–å†…å®¹...")
    
    async with async_playwright() as p:
        # ç™¾åº¦çƒ­æœé€šå¸¸ä¸éœ€è¦å¤æ‚çš„éªŒè¯ï¼Œheadless=True å³å¯
        # Tophub éœ€è¦ headless=False
        headless = source['type'] != 'manual_captcha'
        
        browser = await p.chromium.launch(headless=headless)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            await page.goto(source['url'], timeout=60000)
            
            if source['type'] == 'manual_captcha':
                print("\n" + "="*50)
                print("ã€æ™ºèƒ½ç­‰å¾…åŠ è½½ã€‘")
                print("æ­£åœ¨ç­‰å¾…é¡µé¢åŠ è½½... å¦‚é‡éªŒè¯ç è¯·æ‰‹åŠ¨å®Œæˆã€‚")
                print("="*50 + "\n")
                
                # 1. å°è¯•è‡ªåŠ¨ç­‰å¾…åŠ è½½å®Œæˆ
                try:
                    await page.wait_for_load_state('networkidle', timeout=15000)
                except:
                    print("ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­æ£€æŸ¥å†…å®¹...")

                # 2. æ£€æŸ¥å½“å‰å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                temp_text = await page.inner_text("body")
                if len(temp_text) < 500 or "Just a moment" in temp_text:
                    print("æ£€æµ‹åˆ°å†…å®¹è¿‡çŸ­æˆ–åŒ…å«éªŒè¯æç¤ºï¼Œæš‚åœç­‰å¾…äººå·¥ä»‹å…¥...")
                    await asyncio.to_thread(input, ">> è¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯å¹¶æ˜¾ç¤ºæ–°é—»åˆ—è¡¨åï¼ŒæŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")
                else:
                    print(f"é¡µé¢ä¼¼ä¹å·²åŠ è½½ (å†…å®¹é•¿åº¦: {len(temp_text)})ï¼Œè‡ªåŠ¨ç»§ç»­...")
                
                # === Tophub ç‰¹æ®Šé€»è¾‘ï¼šå°è¯•è·å–æ™šæŠ¥ ===
                if "tophub.today" in source['url']:
                    print("å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰ã€æ™šæŠ¥ã€‘å†…å®¹...")
                    try:
                        content_early = await page.inner_text("body")
                        print(f"ã€å½“å‰æ—©æŠ¥ã€‘å­—æ•°: {len(content_early)}")
                        print("ã€éœ€è¦äººå·¥ä»‹å…¥ã€‘è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨ç‚¹å‡»æ­£ç¡®çš„ã€æ™šæŠ¥ã€‘æŒ‰é’®ï¼Œç¡®è®¤å†…å®¹æ›´æ–°ã€‚")
                        await asyncio.to_thread(input, ">> æ‰‹åŠ¨åˆ‡æ¢å®Œæˆåï¼Œè¯·æŒ‰ã€å›è½¦é”®ã€‘ç»§ç»­...")

                        content_final = await page.inner_text("body")
                        print(f"äººå·¥ç¡®è®¤åå†…å®¹å­—æ•°: {len(content_final)}")

                        if len(content_final) != len(content_early):
                            merged = f"=== æ—©æŠ¥å†…å®¹ ===\n{content_early}\n\n=== æ™šæŠ¥å†…å®¹ ===\n{content_final}"
                            print(f"åˆå¹¶åæ€»å­—æ•°: {len(merged)}")
                            return merged

                        print("å†…å®¹ä»æœªå˜åŒ–ï¼Œå°†ä»…ä½¿ç”¨å½“å‰å†…å®¹ã€‚")
                        return content_final

                    except Exception as e:
                        print(f"å°è¯•åˆ‡æ¢æ™šæŠ¥æ—¶å‡ºé”™: {e}ã€‚å°†ä»…ä½¿ç”¨å½“å‰å†…å®¹ã€‚")
                        pass
                # ========================================

            else:
                # è‡ªåŠ¨ç­‰å¾…
                print("ç­‰å¾…é¡µé¢åŠ è½½...")
                await page.wait_for_load_state('networkidle')
                await page.wait_for_timeout(3000)

            # è·å– HTML
            content = await page.content()
            
            # ä½¿ç”¨ BeautifulSoup æå–ä¸»è¦å†…å®¹ï¼Œå‡å°‘ token
            soup = BeautifulSoup(content, 'html.parser')
            
            # é’ˆå¯¹ä¸åŒæºçš„ç®€å•æ¸…ç†
            if "baidu" in source['url']:
                # ç™¾åº¦çƒ­æœçš„ä¸»è¦å†…å®¹åœ¨ main æˆ–ç‰¹å®š class ä¸­
                main_content = soup.find('div', class_='container') or soup.body
            else:
                main_content = soup.body
                
            # è½¬ä¸ºæ–‡æœ¬ (ä¿ç•™ä¸€å®šçš„ HTML ç»“æ„å¯èƒ½æ›´å¥½ï¼Œä½†çº¯æ–‡æœ¬æ›´çœ token)
            # è¿™é‡Œæˆ‘ä»¬ä¸ºäº†è®© LLM æ›´å¥½ç†è§£ç»“æ„ï¼Œæå–çº¯æ–‡æœ¬ä½†ä¿ç•™æ¢è¡Œ
            text_content = main_content.get_text(separator='\n', strip=True)
            
            print(f"æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(text_content)} å­—ç¬¦")
            return text_content
            
        except Exception as e:
            print(f"è·å–å¤±è´¥: {e}")
            return None
        finally:
            await browser.close()

async def process_with_llm(text):
    """
    è°ƒç”¨ LLM API å¯¹æ–‡æœ¬è¿›è¡Œåˆ†æå’Œæ€»ç»“ (ä»¿ç…§ 60s-static-host æ€è·¯)
    """
    try:
        from openai import OpenAI
    except ImportError:
        print("é”™è¯¯ï¼šæœªå®‰è£… openai åº“ã€‚")
        return None

    client = OpenAI(api_key=API_KEY, base_url=API_BASE_URL)
    
    # è®¡ç®—å‘å¸ƒæ—¥æœŸï¼ˆæ˜å¤©ï¼‰
    publish_date = datetime.datetime.now() + datetime.timedelta(days=1)
    publish_date_str = publish_date.strftime("%Yå¹´%mæœˆ%dæ—¥")

    prompt = f"""
è¯·å°†ä»¥ä¸‹æ–°é—»å†…å®¹æå–ä¸º JSON æ ¼å¼ï¼Œé€‚é…æ—©é—´å…¬ä¼—å·æ–°é—»æ€»ç»“åœºæ™¯ï¼Œçªå‡ºâ€œæƒå¨ã€å®ç”¨ã€æ˜“è¯»â€ã€‚

ã€æ—¥æœŸè§„åˆ™ã€‘
1. æœ¬å†…å®¹å°†äº **{publish_date_str}** å‘å¸ƒã€‚
2. æ–‡ä¸­æ‰€æœ‰æ¶‰åŠæ—¶é—´çš„å†…å®¹ï¼Œ**å¿…é¡»è½¬æ¢ä¸ºç»å¯¹æ—¥æœŸ**ï¼ˆå¦‚â€œ1æœˆ18æ—¥â€ï¼‰ï¼Œ**ä¸¥ç¦**ä½¿ç”¨â€œæ˜æ—¥â€ã€â€œæ˜å¤©â€ã€â€œä¸‹å‘¨â€ã€â€œåå¤©â€ç­‰ç›¸å¯¹æ—¶é—´åè¯ï¼Œé¿å…è¯»è€…äº§ç”Ÿæ—¶é—´é”™ä¹±ã€‚
3. å¦‚æœåŸæ–‡è¯´æ˜¯â€œæ˜å¤©å¼€å§‹â€ï¼Œè¯·æ ¹æ®å½“å‰æ—¥æœŸæ¨ç®—å…·ä½“æ˜¯å“ªä¸€å¤©å¹¶å†™æ˜ã€‚

ã€ç­›é€‰è§„åˆ™ã€‘
1.  ä¼˜å…ˆé€‰æ‹© **æ°‘ç”Ÿæ”¿ç­–ã€è¡Œä¸šå¤§äº‹ã€æ­£èƒ½é‡ç¤¾ä¼šæ–°é—»ã€é‡è¦ç§‘æŠ€/ç»æµåŠ¨æ€**ï¼›
2.  å‰”é™¤å†…å®¹ï¼šç¾éš¾äº‹æ•…ã€è´Ÿé¢æš´åŠ›ã€æ•æ„Ÿæ”¿æ²»ã€æ— æ„ä¹‰å…«å¦ï¼›
3.  ç­›é€‰æ ‡å‡†ï¼šä¼˜å…ˆä»å½“æ—¥çƒ­æœTop30ã€æƒå¨åª’ä½“å¤´ç‰ˆä¸­é€‰å–ï¼Œç¡®ä¿15æ¡å†…å®¹è¦†ç›–å¤šé¢†åŸŸï¼ˆä¸é‡å¤åŒä¸€ä¸»é¢˜ï¼‰ã€‚
4.  å†…å®¹å”¯ä¸€æ€§è¦æ±‚ï¼šä¸¥ç¦å‡ºç°æ ‡é¢˜ã€æ ¸å¿ƒäº‹å®ï¼ˆå¦‚äº‹ä»¶ä¸»ä½“ã€å…³é”®æ•°æ®ã€æ—¶é—´ï¼‰å®Œå…¨ä¸€è‡´çš„é‡å¤å†…å®¹ï¼Œç›¸ä¼¼ä¸»é¢˜éœ€å·®å¼‚åŒ–è¡¨è¿°ï¼ˆå¦‚ä¸åŒåœ°åŒºçš„æ°‘ç”Ÿæ”¿ç­–ï¼‰ï¼›
5.  å…œåº•æœºåˆ¶ï¼šè‹¥å½“æ—¥ç¬¦åˆè¦æ±‚çš„æœ‰æ•ˆæ–°é—»ä¸è¶³15æ¡ï¼ŒæŒ‰å®é™…æ•°é‡è¾“å‡ºï¼ˆæ— éœ€å‡‘æ•°ï¼‰ï¼Œä¼˜å…ˆä¿ç•™é«˜å…³æ³¨åº¦é¢†åŸŸå†…å®¹ã€‚
ã€æ ¼å¼è¦æ±‚ã€‘
1.  æä¾›ä¸€ä¸ªæ ‡é¢˜å­—æ®µ "page_title"ï¼š
    - ç»“æ„ï¼š60 s çœ‹æ‡‚ä¸–ç•Œ +  1-2ä¸ªé«˜å…³æ³¨åº¦å…³é”®è¯
    - å­—æ•°ï¼š18-25å­—ï¼Œæ‰‹æœºç«¯æ˜¾ç¤ºå®Œæ•´ï¼Œé¿å…â€œä»Šæ—¥æ–°é—»æ±‡æ€»â€ç±»å¹³æ·¡è¡¨è¿°
    - é£æ ¼ï¼šç®€æ´æœ‰åŠ›ï¼Œå¸¦æ­£å‘å¼•å¯¼ï¼Œ
2.  ç”Ÿæˆ 1 ä¸ªå¼€å¤´æ–‡æ¡ˆå­—æ®µ "opening"ï¼š
    - é£æ ¼ï¼šå¹²ç»ƒå®ç”¨ï¼Œç¬¦åˆæ—©é—´å¿«é€Ÿè¯»æ–°é—»çš„èŠ‚å¥ï¼Œ30-45å­—
    - å†…å®¹ï¼šæ—¶é—´æé†’+1-2ä¸ªæ ¸å¿ƒæ–°é—»é’©å­+é˜…è¯»å¼•å¯¼ï¼Œç‚¹æ˜â€œé«˜æ•ˆé€Ÿè§ˆã€å…³ä¹æ°‘ç”Ÿâ€çš„å±æ€§
    - ç‚¹ç¼€ï¼šå¯åŠ 1ä¸ªæç¤ºç±»emojiï¼ˆå¦‚ğŸ“Œ/â°ï¼‰
3.  ç”Ÿæˆ 1 ä¸ªç»“å°¾æ–‡æ¡ˆå­—æ®µ "ending"ï¼š
    - é£æ ¼ï¼šæ¸©å’Œæ­£å‘ï¼Œå¸¦å¼•å¯¼æ€§ï¼Œ40-55å­—
    - å†…å®¹ï¼šæ€»ç»“ä»·å€¼+äº’åŠ¨æé—®ï¼ˆå¯é€‰ï¼‰+æ˜Ÿæ ‡å¼•å¯¼
    - ç‚¹ç¼€ï¼šå¯åŠ 1ä¸ªå¼•å¯¼ç±»emojiï¼ˆå¦‚â­/ğŸ””ï¼‰
4.  æå– 15 æ¡æ–°é—»ï¼Œæ”¾å…¥ "news_items" æ•°ç»„ä¸­ï¼ˆä¸è¶³åˆ™æŒ‰å®é™…æ•°é‡è¾“å‡ºï¼‰ã€‚æ¯æ¡å¿…é¡»åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå­—æ®µï¼š
    - "title"ï¼šæ–°é—»æ ‡é¢˜ï¼ˆâ‰¤20å­—ï¼Œç®€ç»ƒæœ‰åŠ›ï¼Œæå–æ ¸å¿ƒäº‹å®/æ•°æ®/çªç ´ç‚¹ï¼Œä¸å †ç Œä¿®é¥°è¯ï¼‰ï¼›
    - "summary_short"ï¼šå›¾ç‰‡é…å¥—æ–‡æ¡ˆï¼ˆ24-32å­—ï¼Œå¿…é¡»ä¸€è¡Œæ˜¾ç¤ºå®Œæ•´ï¼‰ï¼š
        - ç¬¬ä¸€å¥ï¼šç‚¹å‡ºæ–°é—»æ ¸å¿ƒçœ‹ç‚¹/å…³é”®å˜åŒ–ï¼ˆä¸é‡å¤æ ‡é¢˜ï¼‰ï¼›
        - ç¬¬äºŒå¥ï¼šç»™å‡ºä¸€å¥è§£è¯»/å½±å“/å¯ç¤ºï¼ˆå¸¦ä¸€ç‚¹è§‚ç‚¹ï¼‰ï¼›
        - å¥é—´ç”¨â€œï¼›â€åˆ†éš”ï¼Œ
    - "summary_long"ï¼šæ·±åº¦åˆ†æä¸èƒŒæ™¯è¡¥å……ï¼ˆ100-120å­—ï¼‰ï¼š
        - å†…å®¹ï¼šå……å®æœ‰è§‚ç‚¹ï¼ŒåŒ…å«èƒŒæ™¯ã€ç»†èŠ‚ã€å½±å“ï¼›
        - é£æ ¼ï¼šå®¢è§‚æƒå¨ï¼Œé¿å…å£è¯­åŒ–ã€‚
5.  æå–ä¸€å¥æœ€æœ‰å“²ç†çš„â€œé‡‘å¥â€ä½œä¸º "tip"ï¼š
    - é£æ ¼ï¼šæ­£å‘æœ‰æ·±åº¦ï¼Œè´´åˆæ—©é—´é˜…è¯»æ°›å›´ï¼›
    - å­—æ•°ï¼š15-20å­—ï¼Œä¸å¸¦ä½œè€…ï¼Œé¿å…é¸¡æ±¤åŒ–ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- åªè¿”å›çº¯ JSON æ ¼å¼ï¼Œæ— ä»»ä½• Markdown æ ‡è®°ã€æ— å¤šä½™è§£é‡Šæ–‡å­—ï¼›
- è¯­è¨€é£æ ¼ç»Ÿä¸€ï¼šæ­£å¼ä¸”æ˜“æ‡‚ï¼Œé€‚é…æ—©é—´æ–°é—»çš„æƒå¨æ„Ÿä¸å®ç”¨æ€§ã€‚

ã€å†…å®¹ã€‘
{text[:15000]}
"""

    print("æ­£åœ¨è¯·æ±‚ AI è¿›è¡Œæ™ºèƒ½è§£æ...")
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ–°é—»ç¼–è¾‘ä¸è¯„è®ºå‘˜ï¼Œæ“…é•¿ç”¨ç®€çŸ­ä¸¤å¥è¯ç»™å‡ºè®²è§£ä¸è§‚ç‚¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=False
        )
        
        result = response.choices[0].message.content
        result = result.replace("```json", "").replace("```", "").strip()
        
        # é’ˆå¯¹æ€§ä¿®å¤ï¼šåªæœ‰å½“ä¸­æ–‡å¼•å·å‡ºç°åœ¨é”®å€¼å¯¹çš„åˆ†éš”ç¬¦ä½ç½®æ—¶æ‰æ›¿æ¢
        import re
        # æ›¿æ¢é”®å€¼å¯¹å†’å·åçš„å¼€å¼•å·: : â€œ -> : "
        result = re.sub(r':\s*â€œ', ': "', result)
        # æ›¿æ¢é€—å·å‰çš„é—­å¼•å·: â€, -> ",
        result = re.sub(r'â€\s*,', '",', result)
        # æ›¿æ¢å¯¹è±¡ç»“æŸå‰çš„é—­å¼•å·: â€} -> "}
        result = re.sub(r'â€\s*}', '"}', result)
        # æ›¿æ¢åˆ—è¡¨ç»“æŸå‰çš„é—­å¼•å·: â€] -> "]
        result = re.sub(r'â€\s*]', '"]', result)
        
        # print(f"DEBUG: LLM åŸå§‹å“åº”:\n{result}\n")
        
        data = json.loads(result)
        if not isinstance(data, dict):
            data = {"news_items": []}

        if isinstance(data.get("page_title"), dict):
            data["page_title"] = data["page_title"].get("text") or data["page_title"].get("title") or ""
        if "page_title" not in data:
            data["page_title"] = data.get("headline") or data.get("title_text") or ""
        if not isinstance(data.get("page_title"), str):
            data["page_title"] = ""
        data["page_title"] = data["page_title"].strip()
        if len(data["page_title"]) > 25:
            data["page_title"] = data["page_title"][:25]

        if isinstance(data.get("opening"), dict):
            data["opening"] = data["opening"].get("text") or data["opening"].get("content") or ""
        if "opening" not in data:
            data["opening"] = data.get("intro") or data.get("lead") or ""
        if not isinstance(data.get("opening"), str):
            data["opening"] = ""
        data["opening"] = data["opening"].strip()

        if isinstance(data.get("ending"), dict):
            data["ending"] = data["ending"].get("text") or data["ending"].get("content") or ""
        if "ending" not in data:
            data["ending"] = data.get("outro") or data.get("closing") or ""
        if not isinstance(data.get("ending"), str):
            data["ending"] = ""
        data["ending"] = data["ending"].strip()
        
        # å…¼å®¹æ€§å¤„ç†ï¼šå¯»æ‰¾å¯èƒ½çš„åˆ—è¡¨é”®å
        if 'news_items' not in data:
            for key in ['news', 'items', 'list', 'data', 'contents']:
                if key in data and isinstance(data[key], list):
                    data['news_items'] = data[key]
                    break
            
        # æ ‡å‡†åŒ–ï¼šç¡®ä¿ news_items æ˜¯å¯¹è±¡åˆ—è¡¨
        if 'news_items' in data and isinstance(data['news_items'], list):
            normalized_items = []
            for item in data['news_items']:
                if isinstance(item, str):
                    # æ—§æ ¼å¼å…¼å®¹ï¼ˆè™½ç„¶ Prompt è¦æ±‚äº†æ–°æ ¼å¼ï¼Œé˜²ä¸‡ä¸€ï¼‰
                    parts = item.split('ï½œ')
                    title = parts[0]
                    summary = parts[1] if len(parts) > 1 else ""
                    normalized_items.append({
                        "title": title,
                        "summary_short": summary[:30], # æˆªæ–­ä½œä¸ºçŸ­æ‘˜è¦
                        "summary_long": summary # ä½œä¸ºé•¿æ‘˜è¦
                    })
                elif isinstance(item, dict):
                    # ç¡®ä¿å­—æ®µé½å…¨
                    if "summary_short" not in item:
                        item["summary_short"] = item.get("summary", "")[:30]
                    if "summary_long" not in item:
                        item["summary_long"] = item.get("summary", "")
                    normalized_items.append(item)
            data['news_items'] = normalized_items
        
        if not data.get('news_items'):
             print("è­¦å‘Š: æœªèƒ½ä» AI å“åº”ä¸­æå–åˆ°ä»»ä½•æ–°é—»æ¡ç›® (news_items ä¸ºç©º)ã€‚")
             print(f"AI è¿”å›çš„é¡¶å±‚é”®: {list(data.keys())}")

        # å…¼å®¹æ€§å¤„ç†ï¼šå°† tip æ˜ å°„ä¸º quote
        if 'tip' in data and 'quote' not in data:
            data['quote'] = {"text": data['tip'], "author": ""}

        # å…¼å®¹æ€§å¤„ç†ï¼šå¦‚æœ quote æ˜¯å­—ç¬¦ä¸²è€Œä¸æ˜¯å¯¹è±¡
        if isinstance(data.get('quote'), str):
            data['quote'] = {"text": data['quote'], "author": ""}
        
        return data
    except json.JSONDecodeError:
        print(f"JSON è§£æå¤±è´¥ã€‚åŸå§‹å“åº”: {result}")
        # å°è¯•ä½¿ç”¨æ­£åˆ™æå–ç»“æ„åŒ–æ•°æ®ï¼ˆå…œåº•ï¼‰
        try:
            import re
            # å°è¯•åŒ¹é…å®Œæ•´çš„ item ç»“æ„
            item_pattern = r'"title":\s*"(.*?)".*?"summary_short":\s*"(.*?)".*?"summary_long":\s*"(.*?)"'
            matches = re.findall(item_pattern, result, re.S)
            
            if matches:
                print(f"é€šè¿‡æ­£åˆ™æ‰¾å› {len(matches)} æ¡ç»“æ„åŒ–æ–°é—»")
                news_items = []
                for m in matches:
                    news_items.append({
                        "title": m[0],
                        "summary_short": m[1],
                        "summary_long": m[2]
                    })
                
                # å°è¯•æå– tip
                tip_match = re.search(r'"tip":\s*"(.*?)"', result)
                tip_text = tip_match.group(1) if tip_match else "ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ·ã€‚"
                
                return {
                    "news_items": news_items,
                    "quote": {"text": tip_text, "author": ""}
                }
            
            # å¦‚æœç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²å¹¶å°½åŠ›æ‹¼å‡‘
            print("æ­£åˆ™ç»“æ„åŒ–æå–å¤±è´¥ï¼Œå°è¯•æå–æ‰€æœ‰å­—ç¬¦ä¸²...")
            all_strings = re.findall(r'"([^"]+)"', result)
            if all_strings and len(all_strings) > 5:
                 # å‡è®¾å‰å‡ ä¸ªæ˜¯ keysï¼Œæˆ‘ä»¬å¾ˆéš¾å‡†ç¡®æ¢å¤ï¼Œè¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨é¿å…å´©æºƒ
                 return {
                    "news_items": [{"title": "æ•°æ®è§£æé”™è¯¯", "summary_short": "è¯·æ£€æŸ¥æ—¥å¿—", "summary_long": f"åŸå§‹æ•°æ®ç‰‡æ®µ: {all_strings[:3]}..."}],
                    "quote": {"text": "ç³»ç»Ÿæ•…éšœ", "author": "Error"}
                 }
        except Exception as e:
            print(f"å…œåº•è§£æä¹Ÿå¤±è´¥: {e}")
            pass
        return None
    except Exception as e:
        print(f"AI åˆ†æå¤±è´¥: {e}")
        return None

def update_data_json(new_data):
    """
    æ›´æ–° data.json æ–‡ä»¶
    """
    try:
        with open('data.json', 'r', encoding='utf-8') as f:
            current_data = json.load(f)
        
        current_data['news_items'] = new_data.get('news_items', [])
        if isinstance(new_data.get("page_title"), str) and new_data["page_title"].strip():
            current_data["page_title"] = new_data["page_title"].strip()
        if isinstance(new_data.get("opening"), str):
            current_data["opening"] = new_data["opening"].strip()
        if isinstance(new_data.get("ending"), str):
            current_data["ending"] = new_data["ending"].strip()
        new_quote = new_data.get('quote')
        if isinstance(new_quote, str):
            current_data['quote']['text'] = new_quote
        elif isinstance(new_quote, dict):
             current_data['quote'] = new_quote

        today = datetime.datetime.now() + datetime.timedelta(days=1)
        week_list = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
        current_data['date_info']['date_str'] = today.strftime("%Yå¹´%mæœˆ%dæ—¥")
        current_data['date_info']['week_str'] = week_list[today.weekday()]
        
        with open('data.json', 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=4)
            
        return current_data
    except Exception as e:
        print(f"æ›´æ–° data.json å¤±è´¥: {e}")
        return None

async def main():
    # 1. å°è¯•ä»ä¸åŒæºè·å–å†…å®¹
    content = None
    for source in SOURCES:
        content = await fetch_html_content(source)
        if content and len(content) > 500: # ç¡®ä¿è·å–åˆ°äº†è¶³å¤Ÿçš„å†…å®¹
            break
        print(f"[{source['name']}] è·å–å†…å®¹è¿‡å°‘æˆ–å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæº...")
    
    if not content:
        print("æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥ã€‚")
        return

    # 2. å¤„ç†å†…å®¹
    data_processed = await process_with_llm(content)
    if not data_processed:
        print("AI å¤„ç†å¤±è´¥ã€‚")
        return

    # æ›´æ–° JSON
    try:
        current_data = update_data_json(data_processed)
        if current_data:
            print("data.json å·²æ›´æ–°ã€‚")
            
            # === æ–°å¢ï¼šç”Ÿæˆæ¯æ—¥æ–‡ç«  ===
            try:
                data = current_data
                date_str = data['date_info']['date_str']
                # æ›¿æ¢æ—¥æœŸä¸­çš„ä¸­æ–‡å­—ç¬¦ä»¥ç”¨äºæ–‡ä»¶åï¼ˆå¯é€‰ï¼Œè¿™é‡Œç›´æ¥ç”¨ï¼‰
                filename = f"{date_str}.txt"
                article_dir = r"d:\zixun\æ¯æ—¥æ–‡ç« "
                if not os.path.exists(article_dir):
                    os.makedirs(article_dir)
                
                article_path = os.path.join(article_dir, filename)
                
                page_title = data.get("page_title") or f"{date_str} ä»Šæ—¥é€Ÿè§ˆ"
                article_content = f"ã€{page_title}ã€‘\n\n"
                article_content += f"æ—¥æœŸï¼š{date_str}\n"
                article_content += f"ä»Šæ—¥é‡‘å¥ï¼š{data['quote']['text']}\n"
                if data['quote']['author']:
                    article_content += f"â€”â€” {data['quote']['author']}\n"
                article_content += "\n" + "="*30 + "\n\n"

                opening = (data.get("opening") or "").strip()
                if opening:
                    article_content += f"{opening}\n\n"
                
                for i, item in enumerate(data['news_items'], 1):
                    article_content += f"{i}. {item['title']}\n"
                    article_content += f"   {item['summary_long']}\n\n"

                ending = (data.get("ending") or "").strip()
                if ending:
                    article_content += f"{ending}\n"
                
                with open(article_path, 'w', encoding='utf-8') as f:
                    f.write(article_content)
                    
                print(f"æ–‡ç« å·²ç”Ÿæˆ: {article_path}")
            except Exception as e:
                print(f"ç”Ÿæˆæ–‡ç« å¤±è´¥: {e}")
            # ==========================
            
            print("æ­£åœ¨è°ƒç”¨ gen_image.py ç”Ÿæˆå›¾ç‰‡...")
            import gen_image
            await gen_image.main()
            
    except Exception as e:
        print(f"å¤„ç†æµç¨‹å‡ºé”™: {e}")

if __name__ == "__main__":
    asyncio.run(main())
